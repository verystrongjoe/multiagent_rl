train start... 
steps: 2475, episodes: 100, mean episode reward: -1342.799, reward: -32.011, time: 5.142, TD error: 35.117, c_model: 17.792, actorQ: 1.276, a_model: 1.287
steps: 4975, episodes: 200, mean episode reward: -1320.512, reward: -31.218, time: 5.231, TD error: 34.676, c_model: 17.571, actorQ: 1.271, a_model: 1.269
steps: 7475, episodes: 300, mean episode reward: -1367.74, reward: -32.498, time: 5.265, TD error: 34.753, c_model: 17.609, actorQ: 1.285, a_model: 1.269
steps: 9975, episodes: 400, mean episode reward: -1333.267, reward: -31.196, time: 5.247, TD error: 34.592, c_model: 17.527, actorQ: 1.293, a_model: 1.261
steps: 12475, episodes: 500, mean episode reward: -1424.241, reward: -34.47, time: 5.27, TD error: 34.627, c_model: 17.544, actorQ: 1.312, a_model: 1.256
steps: 14975, episodes: 600, mean episode reward: -1363.78, reward: -33.669, time: 5.231, TD error: 34.686, c_model: 17.574, actorQ: 1.342, a_model: 1.253
steps: 17475, episodes: 700, mean episode reward: -1271.375, reward: -32.143, time: 5.274, TD error: 34.622, c_model: 17.544, actorQ: 1.397, a_model: 1.243
steps: 19975, episodes: 800, mean episode reward: -1259.936, reward: -27.35, time: 5.219, TD error: 34.614, c_model: 17.545, actorQ: 1.459, a_model: 1.236
steps: 22475, episodes: 900, mean episode reward: -1197.857, reward: -25.024, time: 5.236, TD error: 34.352, c_model: 17.421, actorQ: 1.53, a_model: 1.223
steps: 24975, episodes: 1000, mean episode reward: -1208.069, reward: -28.184, time: 5.211, TD error: 34.261, c_model: 17.384, actorQ: 1.614, a_model: 1.213
steps: 27475, episodes: 1100, mean episode reward: -1278.58, reward: -32.162, time: 5.255, TD error: 34.074, c_model: 17.301, actorQ: 1.7, a_model: 1.201
steps: 29975, episodes: 1200, mean episode reward: -1307.042, reward: -32.658, time: 5.253, TD error: 33.856, c_model: 17.203, actorQ: 1.786, a_model: 1.187
steps: 32475, episodes: 1300, mean episode reward: -1334.405, reward: -33.359, time: 5.311, TD error: 33.464, c_model: 17.021, actorQ: 1.894, a_model: 1.169
steps: 34975, episodes: 1400, mean episode reward: -1319.373, reward: -32.181, time: 5.293, TD error: 33.212, c_model: 16.906, actorQ: 1.981, a_model: 1.156
steps: 37475, episodes: 1500, mean episode reward: -1130.463, reward: -25.049, time: 5.236, TD error: 33.01, c_model: 16.815, actorQ: 2.067, a_model: 1.145
steps: 39975, episodes: 1600, mean episode reward: -1168.501, reward: -29.391, time: 5.227, TD error: 32.792, c_model: 16.715, actorQ: 2.149, a_model: 1.133
steps: 42475, episodes: 1700, mean episode reward: -1133.126, reward: -27.866, time: 5.229, TD error: 32.518, c_model: 16.586, actorQ: 2.229, a_model: 1.121
steps: 44975, episodes: 1800, mean episode reward: -952.431, reward: -18.607, time: 5.213, TD error: 32.351, c_model: 16.511, actorQ: 2.308, a_model: 1.109
steps: 47475, episodes: 1900, mean episode reward: -743.49, reward: -11.528, time: 5.323, TD error: 31.989, c_model: 16.339, actorQ: 2.399, a_model: 1.091
steps: 49975, episodes: 2000, mean episode reward: -691.931, reward: -10.281, time: 5.264, TD error: 31.633, c_model: 16.167, actorQ: 2.47, a_model: 1.076
steps: 52475, episodes: 2100, mean episode reward: -663.796, reward: -9.326, time: 5.272, TD error: 31.272, c_model: 15.993, actorQ: 2.539, a_model: 1.061
steps: 54975, episodes: 2200, mean episode reward: -728.881, reward: -10.442, time: 5.217, TD error: 31.054, c_model: 15.889, actorQ: 2.607, a_model: 1.047
steps: 57475, episodes: 2300, mean episode reward: -789.217, reward: -11.796, time: 5.661, TD error: 30.747, c_model: 15.741, actorQ: 2.672, a_model: 1.032
steps: 59975, episodes: 2400, mean episode reward: -960.958, reward: -16.547, time: 5.856, TD error: 30.399, c_model: 15.572, actorQ: 2.736, a_model: 1.016
steps: 62475, episodes: 2500, mean episode reward: -1115.686, reward: -23.051, time: 5.332, TD error: 30.056, c_model: 15.406, actorQ: 2.814, a_model: 0.998
steps: 64975, episodes: 2600, mean episode reward: -1164.764, reward: -24.594, time: 5.241, TD error: 29.759, c_model: 15.261, actorQ: 2.875, a_model: 0.983
steps: 67475, episodes: 2700, mean episode reward: -1143.338, reward: -26.385, time: 5.244, TD error: 29.472, c_model: 15.121, actorQ: 2.935, a_model: 0.968
steps: 69975, episodes: 2800, mean episode reward: -1159.226, reward: -27.124, time: 5.244, TD error: 29.211, c_model: 14.994, actorQ: 2.995, a_model: 0.954
steps: 72475, episodes: 2900, mean episode reward: -1128.149, reward: -26.587, time: 5.244, TD error: 28.959, c_model: 14.872, actorQ: 3.054, a_model: 0.94
steps: 74975, episodes: 3000, mean episode reward: -1173.989, reward: -26.787, time: 5.246, TD error: 28.74, c_model: 14.765, actorQ: 3.112, a_model: 0.927
steps: 77475, episodes: 3100, mean episode reward: -1263.059, reward: -28.941, time: 5.291, TD error: 28.451, c_model: 14.624, actorQ: 3.185, a_model: 0.911
steps: 79975, episodes: 3200, mean episode reward: -1417.743, reward: -33.892, time: 5.28, TD error: 28.235, c_model: 14.519, actorQ: 3.242, a_model: 0.899
steps: 82475, episodes: 3300, mean episode reward: -1430.672, reward: -35.49, time: 5.232, TD error: 28.001, c_model: 14.404, actorQ: 3.298, a_model: 0.886
steps: 84975, episodes: 3400, mean episode reward: -1464.37, reward: -33.278, time: 5.221, TD error: 27.802, c_model: 14.307, actorQ: 3.355, a_model: 0.875
steps: 87475, episodes: 3500, mean episode reward: -1406.346, reward: -29.197, time: 5.249, TD error: 27.614, c_model: 14.215, actorQ: 3.41, a_model: 0.863
steps: 89975, episodes: 3600, mean episode reward: -1430.548, reward: -31.238, time: 5.202, TD error: 27.42, c_model: 14.12, actorQ: 3.466, a_model: 0.852
steps: 92475, episodes: 3700, mean episode reward: -1458.339, reward: -33.534, time: 5.281, TD error: 27.211, c_model: 14.017, actorQ: 3.536, a_model: 0.839
steps: 94975, episodes: 3800, mean episode reward: -1406.822, reward: -32.56, time: 5.262, TD error: 27.085, c_model: 13.956, actorQ: 3.591, a_model: 0.829
steps: 97475, episodes: 3900, mean episode reward: -1415.07, reward: -34.629, time: 5.274, TD error: 26.929, c_model: 13.88, actorQ: 3.646, a_model: 0.82
steps: 99975, episodes: 4000, mean episode reward: -1465.449, reward: -36.965, time: 5.237, TD error: 26.769, c_model: 13.801, actorQ: 3.701, a_model: 0.81
steps: 102475, episodes: 4100, mean episode reward: -1455.235, reward: -36.805, time: 5.244, TD error: 26.621, c_model: 13.728, actorQ: 3.756, a_model: 0.801
steps: 104975, episodes: 4200, mean episode reward: -1470.922, reward: -35.179, time: 5.264, TD error: 26.516, c_model: 13.677, actorQ: 3.81, a_model: 0.792
steps: 107475, episodes: 4300, mean episode reward: -1500.146, reward: -37.556, time: 5.307, TD error: 26.345, c_model: 13.593, actorQ: 3.878, a_model: 0.782
steps: 109975, episodes: 4400, mean episode reward: -1495.518, reward: -38.644, time: 5.247, TD error: 26.244, c_model: 13.543, actorQ: 3.933, a_model: 0.774
steps: 112475, episodes: 4500, mean episode reward: -1468.023, reward: -38.172, time: 5.232, TD error: 26.1, c_model: 13.472, actorQ: 3.987, a_model: 0.766
steps: 114975, episodes: 4600, mean episode reward: -1517.357, reward: -38.908, time: 5.294, TD error: 25.99, c_model: 13.417, actorQ: 4.04, a_model: 0.758
steps: 117475, episodes: 4700, mean episode reward: -1497.993, reward: -38.734, time: 5.233, TD error: 25.885, c_model: 13.366, actorQ: 4.094, a_model: 0.75
steps: 119975, episodes: 4800, mean episode reward: -1499.369, reward: -38.667, time: 5.239, TD error: 25.781, c_model: 13.314, actorQ: 4.148, a_model: 0.743
steps: 122475, episodes: 4900, mean episode reward: -1509.159, reward: -38.755, time: 5.279, TD error: 25.638, c_model: 13.243, actorQ: 4.214, a_model: 0.734
steps: 124975, episodes: 5000, mean episode reward: -1475.827, reward: -38.216, time: 5.215, TD error: 25.56, c_model: 13.204, actorQ: 4.267, a_model: 0.727
steps: 127475, episodes: 5100, mean episode reward: -1489.759, reward: -38.732, time: 5.209, TD error: 25.454, c_model: 13.151, actorQ: 4.321, a_model: 0.721
steps: 129975, episodes: 5200, mean episode reward: -1525.409, reward: -39.062, time: 5.212, TD error: 25.332, c_model: 13.09, actorQ: 4.373, a_model: 0.714
steps: 132475, episodes: 5300, mean episode reward: -1493.935, reward: -38.709, time: 5.224, TD error: 25.266, c_model: 13.057, actorQ: 4.426, a_model: 0.707
steps: 134975, episodes: 5400, mean episode reward: -1290.886, reward: -33.064, time: 5.256, TD error: 25.183, c_model: 13.015, actorQ: 4.479, a_model: 0.701
steps: 137475, episodes: 5500, mean episode reward: -1511.903, reward: -38.547, time: 5.298, TD error: 25.101, c_model: 12.974, actorQ: 4.544, a_model: 0.693
steps: 139975, episodes: 5600, mean episode reward: -1448.433, reward: -38.112, time: 5.22, TD error: 25.027, c_model: 12.936, actorQ: 4.597, a_model: 0.687
steps: 142475, episodes: 5700, mean episode reward: -1504.063, reward: -37.774, time: 5.239, TD error: 24.925, c_model: 12.885, actorQ: 4.649, a_model: 0.681
steps: 144975, episodes: 5800, mean episode reward: -1423.333, reward: -35.642, time: 5.343, TD error: 24.818, c_model: 12.831, actorQ: 4.701, a_model: 0.675
steps: 147475, episodes: 5900, mean episode reward: -1312.479, reward: -30.726, time: 5.249, TD error: 24.727, c_model: 12.785, actorQ: 4.753, a_model: 0.67
steps: 149975, episodes: 6000, mean episode reward: -1505.242, reward: -38.932, time: 5.416, TD error: 24.635, c_model: 12.738, actorQ: 4.805, a_model: 0.664
steps: 152475, episodes: 6100, mean episode reward: -1437.96, reward: -37.253, time: 5.314, TD error: 24.516, c_model: 12.677, actorQ: 4.87, a_model: 0.657
steps: 154975, episodes: 6200, mean episode reward: -1361.644, reward: -34.089, time: 5.244, TD error: 24.434, c_model: 12.635, actorQ: 4.922, a_model: 0.652
steps: 157475, episodes: 6300, mean episode reward: -1389.671, reward: -35.35, time: 5.278, TD error: 24.351, c_model: 12.592, actorQ: 4.973, a_model: 0.647
steps: 159975, episodes: 6400, mean episode reward: -1406.238, reward: -35.148, time: 5.25, TD error: 24.28, c_model: 12.556, actorQ: 5.025, a_model: 0.642
steps: 162475, episodes: 6500, mean episode reward: -1479.871, reward: -38.006, time: 5.275, TD error: 24.2, c_model: 12.514, actorQ: 5.076, a_model: 0.636
steps: 164975, episodes: 6600, mean episode reward: -1468.813, reward: -37.81, time: 5.276, TD error: 24.133, c_model: 12.479, actorQ: 5.128, a_model: 0.632
steps: 167475, episodes: 6700, mean episode reward: -1468.242, reward: -37.795, time: 5.297, TD error: 24.02, c_model: 12.421, actorQ: 5.192, a_model: 0.625
steps: 169975, episodes: 6800, mean episode reward: -1481.256, reward: -38.133, time: 5.264, TD error: 23.936, c_model: 12.377, actorQ: 5.243, a_model: 0.621
steps: 172475, episodes: 6900, mean episode reward: -1461.786, reward: -36.578, time: 5.255, TD error: 23.875, c_model: 12.345, actorQ: 5.294, a_model: 0.616
steps: 174975, episodes: 7000, mean episode reward: -1302.285, reward: -30.543, time: 5.207, TD error: 23.799, c_model: 12.305, actorQ: 5.345, a_model: 0.611
steps: 177475, episodes: 7100, mean episode reward: -1364.963, reward: -32.596, time: 5.226, TD error: 23.712, c_model: 12.26, actorQ: 5.396, a_model: 0.606
steps: 179975, episodes: 7200, mean episode reward: -1336.423, reward: -31.945, time: 5.259, TD error: 23.627, c_model: 12.216, actorQ: 5.447, a_model: 0.602
steps: 182475, episodes: 7300, mean episode reward: -1488.367, reward: -37.429, time: 5.282, TD error: 23.533, c_model: 12.166, actorQ: 5.511, a_model: 0.596
steps: 184975, episodes: 7400, mean episode reward: -1510.178, reward: -39.003, time: 5.224, TD error: 23.449, c_model: 12.122, actorQ: 5.562, a_model: 0.592
steps: 187475, episodes: 7500, mean episode reward: -1337.023, reward: -31.477, time: 5.27, TD error: 23.386, c_model: 12.089, actorQ: 5.613, a_model: 0.588
steps: 189975, episodes: 7600, mean episode reward: -1191.93, reward: -27.877, time: 5.217, TD error: 23.316, c_model: 12.052, actorQ: 5.664, a_model: 0.583
steps: 192475, episodes: 7700, mean episode reward: -1446.903, reward: -36.276, time: 5.227, TD error: 23.237, c_model: 12.01, actorQ: 5.714, a_model: 0.579
steps: 194975, episodes: 7800, mean episode reward: -1459.144, reward: -37.279, time: 5.237, TD error: 23.168, c_model: 11.973, actorQ: 5.765, a_model: 0.575
steps: 197475, episodes: 7900, mean episode reward: -1499.892, reward: -38.595, time: 5.292, TD error: 23.067, c_model: 11.92, actorQ: 5.828, a_model: 0.57
steps: 199975, episodes: 8000, mean episode reward: -1488.336, reward: -37.844, time: 5.214, TD error: 22.995, c_model: 11.882, actorQ: 5.879, a_model: 0.566
steps: 202475, episodes: 8100, mean episode reward: -1316.933, reward: -32.35, time: 5.285, TD error: 22.939, c_model: 11.851, actorQ: 5.93, a_model: 0.562
steps: 204975, episodes: 8200, mean episode reward: -1256.283, reward: -28.613, time: 5.841, TD error: 22.882, c_model: 11.82, actorQ: 5.98, a_model: 0.558
steps: 207475, episodes: 8300, mean episode reward: -1347.403, reward: -32.441, time: 5.661, TD error: 22.808, c_model: 11.78, actorQ: 6.031, a_model: 0.554
steps: 209975, episodes: 8400, mean episode reward: -1467.956, reward: -34.676, time: 5.218, TD error: 22.741, c_model: 11.744, actorQ: 6.082, a_model: 0.55
steps: 212475, episodes: 8500, mean episode reward: -1490.653, reward: -37.632, time: 5.274, TD error: 22.656, c_model: 11.698, actorQ: 6.145, a_model: 0.545
steps: 214975, episodes: 8600, mean episode reward: -1337.958, reward: -30.496, time: 5.21, TD error: 22.589, c_model: 11.662, actorQ: 6.195, a_model: 0.541
steps: 217475, episodes: 8700, mean episode reward: -1517.171, reward: -38.597, time: 5.233, TD error: 22.515, c_model: 11.623, actorQ: 6.246, a_model: 0.537
steps: 219975, episodes: 8800, mean episode reward: -1488.635, reward: -37.196, time: 5.228, TD error: 22.437, c_model: 11.581, actorQ: 6.296, a_model: 0.534
steps: 222475, episodes: 8900, mean episode reward: -1462.769, reward: -37.419, time: 5.218, TD error: 22.374, c_model: 11.547, actorQ: 6.347, a_model: 0.53
steps: 224975, episodes: 9000, mean episode reward: -1366.369, reward: -33.939, time: 5.248, TD error: 22.313, c_model: 11.513, actorQ: 6.397, a_model: 0.526
steps: 227475, episodes: 9100, mean episode reward: -1219.682, reward: -28.113, time: 5.306, TD error: 22.225, c_model: 11.466, actorQ: 6.46, a_model: 0.522
steps: 229975, episodes: 9200, mean episode reward: -1281.374, reward: -30.27, time: 5.248, TD error: 22.166, c_model: 11.434, actorQ: 6.511, a_model: 0.518
steps: 232475, episodes: 9300, mean episode reward: -1453.915, reward: -36.647, time: 5.71, TD error: 22.103, c_model: 11.4, actorQ: 6.562, a_model: 0.515
steps: 234975, episodes: 9400, mean episode reward: -1414.725, reward: -35.378, time: 5.264, TD error: 22.049, c_model: 11.37, actorQ: 6.612, a_model: 0.511
steps: 237475, episodes: 9500, mean episode reward: -1388.314, reward: -34.579, time: 5.246, TD error: 21.983, c_model: 11.334, actorQ: 6.663, a_model: 0.508
steps: 239975, episodes: 9600, mean episode reward: -1372.799, reward: -33.161, time: 5.24, TD error: 21.927, c_model: 11.303, actorQ: 6.714, a_model: 0.505
steps: 242475, episodes: 9700, mean episode reward: -1329.756, reward: -30.285, time: 5.288, TD error: 21.841, c_model: 11.257, actorQ: 6.777, a_model: 0.5
steps: 244975, episodes: 9800, mean episode reward: -1475.334, reward: -37.058, time: 5.202, TD error: 21.785, c_model: 11.226, actorQ: 6.828, a_model: 0.497
steps: 247475, episodes: 9900, mean episode reward: -1466.334, reward: -37.169, time: 5.227, TD error: 21.73, c_model: 11.195, actorQ: 6.879, a_model: 0.494
steps: 249975, episodes: 10000, mean episode reward: -1417.371, reward: -36.782, time: 5.228, TD error: 21.679, c_model: 11.167, actorQ: 6.93, a_model: 0.491
steps: 252475, episodes: 10100, mean episode reward: -1439.638, reward: -36.635, time: 5.275, TD error: 21.615, c_model: 11.132, actorQ: 6.981, a_model: 0.488
steps: 254975, episodes: 10200, mean episode reward: -1203.711, reward: -28.063, time: 5.2, TD error: 21.558, c_model: 11.101, actorQ: 7.032, a_model: 0.485
steps: 257475, episodes: 10300, mean episode reward: -1344.35, reward: -33.162, time: 5.285, TD error: 21.495, c_model: 11.066, actorQ: 7.096, a_model: 0.481
steps: 259975, episodes: 10400, mean episode reward: -1406.118, reward: -35.054, time: 5.44, TD error: 21.448, c_model: 11.04, actorQ: 7.147, a_model: 0.478
steps: 262475, episodes: 10500, mean episode reward: -1468.388, reward: -38.162, time: 5.244, TD error: 21.407, c_model: 11.016, actorQ: 7.198, a_model: 0.475
steps: 264975, episodes: 10600, mean episode reward: -1464.404, reward: -37.74, time: 5.25, TD error: 21.359, c_model: 10.989, actorQ: 7.25, a_model: 0.472
steps: 267475, episodes: 10700, mean episode reward: -1480.763, reward: -36.705, time: 5.255, TD error: 21.309, c_model: 10.961, actorQ: 7.301, a_model: 0.469
steps: 269975, episodes: 10800, mean episode reward: -1357.538, reward: -33.303, time: 5.258, TD error: 21.257, c_model: 10.932, actorQ: 7.352, a_model: 0.466
steps: 272475, episodes: 10900, mean episode reward: -1262.086, reward: -29.31, time: 5.239, TD error: 21.189, c_model: 10.895, actorQ: 7.417, a_model: 0.463
steps: 274975, episodes: 11000, mean episode reward: -1414.538, reward: -34.266, time: 5.221, TD error: 21.153, c_model: 10.874, actorQ: 7.468, a_model: 0.46
steps: 277475, episodes: 11100, mean episode reward: -1410.732, reward: -33.119, time: 5.25, TD error: 21.107, c_model: 10.849, actorQ: 7.52, a_model: 0.457
steps: 279975, episodes: 11200, mean episode reward: -1309.693, reward: -31.972, time: 5.197, TD error: 21.063, c_model: 10.824, actorQ: 7.572, a_model: 0.455
steps: 282475, episodes: 11300, mean episode reward: -1456.146, reward: -36.664, time: 5.239, TD error: 21.024, c_model: 10.802, actorQ: 7.623, a_model: 0.452
steps: 284975, episodes: 11400, mean episode reward: -1433.932, reward: -37.448, time: 5.268, TD error: 20.979, c_model: 10.777, actorQ: 7.675, a_model: 0.45
steps: 287475, episodes: 11500, mean episode reward: -1421.19, reward: -36.601, time: 5.266, TD error: 20.928, c_model: 10.748, actorQ: 7.74, a_model: 0.446
steps: 289975, episodes: 11600, mean episode reward: -1340.075, reward: -33.506, time: 5.204, TD error: 20.873, c_model: 10.719, actorQ: 7.792, a_model: 0.444
steps: 292475, episodes: 11700, mean episode reward: -1254.088, reward: -28.75, time: 5.247, TD error: 20.833, c_model: 10.697, actorQ: 7.844, a_model: 0.441
steps: 294975, episodes: 11800, mean episode reward: -1286.343, reward: -31.124, time: 5.258, TD error: 20.791, c_model: 10.673, actorQ: 7.897, a_model: 0.439
steps: 297475, episodes: 11900, mean episode reward: -1432.909, reward: -35.209, time: 5.202, TD error: 20.747, c_model: 10.648, actorQ: 7.949, a_model: 0.437
steps: 299975, episodes: 12000, mean episode reward: -1463.899, reward: -37.514, time: 5.219, TD error: 20.699, c_model: 10.622, actorQ: 8.001, a_model: 0.434
steps: 302475, episodes: 12100, mean episode reward: -1276.69, reward: -31.813, time: 5.254, TD error: 20.649, c_model: 10.594, actorQ: 8.067, a_model: 0.431
steps: 304975, episodes: 12200, mean episode reward: -1501.297, reward: -38.804, time: 5.304, TD error: 20.613, c_model: 10.574, actorQ: 8.119, a_model: 0.429
steps: 307475, episodes: 12300, mean episode reward: -1480.803, reward: -38.485, time: 5.602, TD error: 20.573, c_model: 10.552, actorQ: 8.172, a_model: 0.427
steps: 309975, episodes: 12400, mean episode reward: -1471.263, reward: -38.245, time: 5.63, TD error: 20.53, c_model: 10.529, actorQ: 8.225, a_model: 0.424
steps: 312475, episodes: 12500, mean episode reward: -1485.85, reward: -38.37, time: 5.254, TD error: 20.492, c_model: 10.508, actorQ: 8.278, a_model: 0.422
steps: 314975, episodes: 12600, mean episode reward: -1492.372, reward: -38.664, time: 5.38, TD error: 20.453, c_model: 10.486, actorQ: 8.331, a_model: 0.42
steps: 317475, episodes: 12700, mean episode reward: -1502.569, reward: -38.631, time: 5.328, TD error: 20.404, c_model: 10.46, actorQ: 8.397, a_model: 0.417
steps: 319975, episodes: 12800, mean episode reward: -1464.129, reward: -38.04, time: 5.234, TD error: 20.369, c_model: 10.44, actorQ: 8.45, a_model: 0.415
steps: 322475, episodes: 12900, mean episode reward: -1507.763, reward: -38.801, time: 5.223, TD error: 20.334, c_model: 10.42, actorQ: 8.503, a_model: 0.413
steps: 324975, episodes: 13000, mean episode reward: -1496.127, reward: -38.569, time: 5.257, TD error: 20.296, c_model: 10.4, actorQ: 8.556, a_model: 0.411
steps: 327475, episodes: 13100, mean episode reward: -1527.612, reward: -39.274, time: 5.396, TD error: 20.263, c_model: 10.381, actorQ: 8.609, a_model: 0.409
steps: 329975, episodes: 13200, mean episode reward: -1475.408, reward: -38.039, time: 5.238, TD error: 20.221, c_model: 10.359, actorQ: 8.661, a_model: 0.407
steps: 332475, episodes: 13300, mean episode reward: -1506.239, reward: -38.86, time: 5.28, TD error: 20.179, c_model: 10.336, actorQ: 8.725, a_model: 0.404
steps: 334975, episodes: 13400, mean episode reward: -1463.107, reward: -38.01, time: 5.246, TD error: 20.141, c_model: 10.316, actorQ: 8.775, a_model: 0.402
steps: 337475, episodes: 13500, mean episode reward: -1536.691, reward: -39.112, time: 5.235, TD error: 20.1, c_model: 10.294, actorQ: 8.826, a_model: 0.4
steps: 339975, episodes: 13600, mean episode reward: -1444.193, reward: -37.851, time: 5.218, TD error: 20.064, c_model: 10.274, actorQ: 8.877, a_model: 0.398
steps: 342475, episodes: 13700, mean episode reward: -1241.362, reward: -27.376, time: 5.284, TD error: 20.031, c_model: 10.256, actorQ: 8.928, a_model: 0.396
steps: 344975, episodes: 13800, mean episode reward: -824.854, reward: -13.375, time: 5.222, TD error: 19.987, c_model: 10.233, actorQ: 8.978, a_model: 0.394
steps: 347475, episodes: 13900, mean episode reward: -948.754, reward: -16.522, time: 5.278, TD error: 19.943, c_model: 10.21, actorQ: 9.038, a_model: 0.392
steps: 349975, episodes: 14000, mean episode reward: -943.89, reward: -16.859, time: 5.459, TD error: 19.91, c_model: 10.192, actorQ: 9.082, a_model: 0.39
steps: 352475, episodes: 14100, mean episode reward: -699.187, reward: -10.505, time: 5.761, TD error: 19.865, c_model: 10.167, actorQ: 9.12, a_model: 0.388
steps: 354975, episodes: 14200, mean episode reward: -1059.411, reward: -21.161, time: 5.421, TD error: 19.816, c_model: 10.141, actorQ: 9.16, a_model: 0.387
steps: 357475, episodes: 14300, mean episode reward: -1370.332, reward: -31.427, time: 5.213, TD error: 19.769, c_model: 10.116, actorQ: 9.202, a_model: 0.385
steps: 359975, episodes: 14400, mean episode reward: -1399.524, reward: -31.636, time: 5.253, TD error: 19.727, c_model: 10.092, actorQ: 9.244, a_model: 0.383
steps: 362475, episodes: 14500, mean episode reward: -1166.198, reward: -23.428, time: 5.267, TD error: 19.667, c_model: 10.06, actorQ: 9.299, a_model: 0.381
steps: 364975, episodes: 14600, mean episode reward: -806.398, reward: -13.479, time: 5.185, TD error: 19.624, c_model: 10.036, actorQ: 9.343, a_model: 0.379
steps: 367475, episodes: 14700, mean episode reward: -808.573, reward: -13.029, time: 5.211, TD error: 19.575, c_model: 10.01, actorQ: 9.385, a_model: 0.378
steps: 369975, episodes: 14800, mean episode reward: -1027.401, reward: -19.538, time: 5.23, TD error: 19.523, c_model: 9.983, actorQ: 9.43, a_model: 0.376
steps: 372475, episodes: 14900, mean episode reward: -1315.492, reward: -27.688, time: 5.245, TD error: 19.478, c_model: 9.959, actorQ: 9.475, a_model: 0.374
steps: 374975, episodes: 15000, mean episode reward: -1418.246, reward: -33.739, time: 5.241, TD error: 19.435, c_model: 9.936, actorQ: 9.52, a_model: 0.373
steps: 377475, episodes: 15100, mean episode reward: -1430.162, reward: -35.166, time: 5.288, TD error: 19.371, c_model: 9.903, actorQ: 9.576, a_model: 0.371
steps: 379975, episodes: 15200, mean episode reward: -1435.234, reward: -33.486, time: 5.249, TD error: 19.321, c_model: 9.876, actorQ: 9.622, a_model: 0.369
steps: 382475, episodes: 15300, mean episode reward: -1357.583, reward: -29.801, time: 5.24, TD error: 19.264, c_model: 9.847, actorQ: 9.666, a_model: 0.368
steps: 384975, episodes: 15400, mean episode reward: -1259.447, reward: -25.853, time: 5.232, TD error: 19.216, c_model: 9.822, actorQ: 9.714, a_model: 0.366
steps: 387475, episodes: 15500, mean episode reward: -1208.737, reward: -23.316, time: 5.226, TD error: 19.17, c_model: 9.797, actorQ: 9.762, a_model: 0.365
steps: 389975, episodes: 15600, mean episode reward: -1191.116, reward: -23.183, time: 5.213, TD error: 19.118, c_model: 9.77, actorQ: 9.807, a_model: 0.363
steps: 392475, episodes: 15700, mean episode reward: -1227.354, reward: -24.734, time: 5.265, TD error: 19.053, c_model: 9.736, actorQ: 9.867, a_model: 0.361
steps: 394975, episodes: 15800, mean episode reward: -1337.067, reward: -28.921, time: 5.209, TD error: 19.007, c_model: 9.712, actorQ: 9.915, a_model: 0.36
steps: 397475, episodes: 15900, mean episode reward: -1398.418, reward: -31.999, time: 5.216, TD error: 18.959, c_model: 9.687, actorQ: 9.961, a_model: 0.358
steps: 399975, episodes: 16000, mean episode reward: -1413.356, reward: -34.457, time: 5.215, TD error: 18.905, c_model: 9.659, actorQ: 10.008, a_model: 0.357
steps: 402475, episodes: 16100, mean episode reward: -1475.628, reward: -36.631, time: 5.242, TD error: 18.857, c_model: 9.634, actorQ: 10.056, a_model: 0.355
steps: 404975, episodes: 16200, mean episode reward: -1485.612, reward: -36.546, time: 5.219, TD error: 18.81, c_model: 9.61, actorQ: 10.101, a_model: 0.354
steps: 407475, episodes: 16300, mean episode reward: -1459.195, reward: -36.885, time: 5.268, TD error: 18.746, c_model: 9.577, actorQ: 10.159, a_model: 0.352
steps: 409975, episodes: 16400, mean episode reward: -1406.631, reward: -36.984, time: 5.493, TD error: 18.694, c_model: 9.55, actorQ: 10.206, a_model: 0.351
steps: 412475, episodes: 16500, mean episode reward: -1514.463, reward: -39.062, time: 5.252, TD error: 18.647, c_model: 9.526, actorQ: 10.254, a_model: 0.349
steps: 414975, episodes: 16600, mean episode reward: -1497.016, reward: -38.556, time: 5.243, TD error: 18.596, c_model: 9.499, actorQ: 10.301, a_model: 0.348
steps: 417475, episodes: 16700, mean episode reward: -1447.52, reward: -37.757, time: 5.245, TD error: 18.551, c_model: 9.476, actorQ: 10.349, a_model: 0.347
steps: 419975, episodes: 16800, mean episode reward: -1504.599, reward: -38.862, time: 5.245, TD error: 18.501, c_model: 9.45, actorQ: 10.395, a_model: 0.345
steps: 422475, episodes: 16900, mean episode reward: -1525.218, reward: -39.096, time: 5.295, TD error: 18.439, c_model: 9.418, actorQ: 10.455, a_model: 0.344
steps: 424975, episodes: 17000, mean episode reward: -1527.698, reward: -39.292, time: 5.242, TD error: 18.391, c_model: 9.393, actorQ: 10.503, a_model: 0.342
steps: 427475, episodes: 17100, mean episode reward: -1460.65, reward: -38.081, time: 5.245, TD error: 18.344, c_model: 9.369, actorQ: 10.551, a_model: 0.341
steps: 429975, episodes: 17200, mean episode reward: -1390.986, reward: -36.275, time: 5.283, TD error: 18.297, c_model: 9.345, actorQ: 10.599, a_model: 0.34
steps: 432475, episodes: 17300, mean episode reward: -1350.031, reward: -35.355, time: 5.203, TD error: 18.247, c_model: 9.32, actorQ: 10.649, a_model: 0.339
steps: 434975, episodes: 17400, mean episode reward: -1428.846, reward: -36.961, time: 5.24, TD error: 18.2, c_model: 9.295, actorQ: 10.696, a_model: 0.337
steps: 437475, episodes: 17500, mean episode reward: -1421.413, reward: -36.912, time: 5.291, TD error: 18.138, c_model: 9.263, actorQ: 10.755, a_model: 0.336
steps: 439975, episodes: 17600, mean episode reward: -1358.827, reward: -36.016, time: 5.227, TD error: 18.092, c_model: 9.24, actorQ: 10.805, a_model: 0.335
steps: 442475, episodes: 17700, mean episode reward: -1309.641, reward: -34.695, time: 5.254, TD error: 18.045, c_model: 9.216, actorQ: 10.852, a_model: 0.333
steps: 444975, episodes: 17800, mean episode reward: -1343.073, reward: -35.503, time: 5.229, TD error: 17.996, c_model: 9.191, actorQ: 10.898, a_model: 0.332
steps: 447475, episodes: 17900, mean episode reward: -1288.449, reward: -33.876, time: 5.26, TD error: 17.949, c_model: 9.167, actorQ: 10.945, a_model: 0.331
steps: 449975, episodes: 18000, mean episode reward: -1238.616, reward: -32.748, time: 5.202, TD error: 17.898, c_model: 9.141, actorQ: 10.991, a_model: 0.33
steps: 452475, episodes: 18100, mean episode reward: -1246.495, reward: -32.981, time: 5.53, TD error: 17.838, c_model: 9.11, actorQ: 11.052, a_model: 0.328
steps: 454975, episodes: 18200, mean episode reward: -1308.697, reward: -34.372, time: 5.333, TD error: 17.791, c_model: 9.086, actorQ: 11.102, a_model: 0.327
steps: 457475, episodes: 18300, mean episode reward: -1352.157, reward: -35.642, time: 5.497, TD error: 17.741, c_model: 9.061, actorQ: 11.149, a_model: 0.326
steps: 459975, episodes: 18400, mean episode reward: -1311.061, reward: -34.338, time: 5.588, TD error: 17.693, c_model: 9.036, actorQ: 11.198, a_model: 0.325
steps: 462475, episodes: 18500, mean episode reward: -1316.08, reward: -34.615, time: 5.684, TD error: 17.646, c_model: 9.012, actorQ: 11.245, a_model: 0.324
steps: 464975, episodes: 18600, mean episode reward: -1378.48, reward: -36.172, time: 5.486, TD error: 17.594, c_model: 8.985, actorQ: 11.291, a_model: 0.323
steps: 467475, episodes: 18700, mean episode reward: -1411.264, reward: -37.017, time: 5.591, TD error: 17.535, c_model: 8.955, actorQ: 11.351, a_model: 0.321
steps: 469975, episodes: 18800, mean episode reward: -1354.393, reward: -35.922, time: 5.273, TD error: 17.492, c_model: 8.933, actorQ: 11.396, a_model: 0.32
steps: 472475, episodes: 18900, mean episode reward: -1441.718, reward: -37.598, time: 5.239, TD error: 17.444, c_model: 8.909, actorQ: 11.442, a_model: 0.319
steps: 474975, episodes: 19000, mean episode reward: -1437.76, reward: -37.356, time: 5.308, TD error: 17.397, c_model: 8.885, actorQ: 11.49, a_model: 0.318
steps: 477475, episodes: 19100, mean episode reward: -1355.313, reward: -35.736, time: 5.253, TD error: 17.356, c_model: 8.864, actorQ: 11.537, a_model: 0.317
steps: 479975, episodes: 19200, mean episode reward: -1398.358, reward: -36.526, time: 5.342, TD error: 17.313, c_model: 8.842, actorQ: 11.582, a_model: 0.316
steps: 482475, episodes: 19300, mean episode reward: -1364.625, reward: -35.982, time: 5.314, TD error: 17.259, c_model: 8.814, actorQ: 11.642, a_model: 0.315
steps: 484975, episodes: 19400, mean episode reward: -1356.133, reward: -35.716, time: 5.264, TD error: 17.216, c_model: 8.792, actorQ: 11.69, a_model: 0.314
steps: 487475, episodes: 19500, mean episode reward: -1337.232, reward: -34.957, time: 5.276, TD error: 17.172, c_model: 8.769, actorQ: 11.736, a_model: 0.313
steps: 489975, episodes: 19600, mean episode reward: -1421.972, reward: -36.862, time: 5.209, TD error: 17.131, c_model: 8.749, actorQ: 11.784, a_model: 0.312
steps: 492475, episodes: 19700, mean episode reward: -1330.902, reward: -35.254, time: 5.225, TD error: 17.09, c_model: 8.728, actorQ: 11.832, a_model: 0.311
steps: 494975, episodes: 19800, mean episode reward: -1339.057, reward: -35.473, time: 5.269, TD error: 17.049, c_model: 8.707, actorQ: 11.88, a_model: 0.31
steps: 497475, episodes: 19900, mean episode reward: -1347.875, reward: -34.546, time: 5.779, TD error: 16.996, c_model: 8.68, actorQ: 11.937, a_model: 0.308
steps: 499975, episodes: 20000, mean episode reward: -1232.216, reward: -30.414, time: 5.683, TD error: 16.957, c_model: 8.659, actorQ: 11.983, a_model: 0.307
steps: 502475, episodes: 20100, mean episode reward: -1188.009, reward: -28.658, time: 5.278, TD error: 16.918, c_model: 8.639, actorQ: 12.03, a_model: 0.307
steps: 504975, episodes: 20200, mean episode reward: -1078.688, reward: -24.637, time: 5.255, TD error: 16.879, c_model: 8.619, actorQ: 12.077, a_model: 0.306
steps: 507475, episodes: 20300, mean episode reward: -1047.641, reward: -22.129, time: 5.275, TD error: 16.839, c_model: 8.598, actorQ: 12.122, a_model: 0.305
steps: 509975, episodes: 20400, mean episode reward: -1050.156, reward: -22.292, time: 5.221, TD error: 16.792, c_model: 8.575, actorQ: 12.169, a_model: 0.304
steps: 512475, episodes: 20500, mean episode reward: -1078.888, reward: -22.43, time: 5.29, TD error: 16.739, c_model: 8.548, actorQ: 12.224, a_model: 0.302
steps: 514975, episodes: 20600, mean episode reward: -1017.61, reward: -20.681, time: 5.551, TD error: 16.701, c_model: 8.528, actorQ: 12.271, a_model: 0.302
steps: 517475, episodes: 20700, mean episode reward: -1024.416, reward: -21.334, time: 5.229, TD error: 16.663, c_model: 8.509, actorQ: 12.317, a_model: 0.301
steps: 519975, episodes: 20800, mean episode reward: -1078.46, reward: -23.765, time: 5.263, TD error: 16.624, c_model: 8.489, actorQ: 12.362, a_model: 0.3
steps: 522475, episodes: 20900, mean episode reward: -1026.665, reward: -21.344, time: 5.263, TD error: 16.585, c_model: 8.469, actorQ: 12.405, a_model: 0.299
steps: 524975, episodes: 21000, mean episode reward: -963.604, reward: -19.226, time: 5.237, TD error: 16.548, c_model: 8.45, actorQ: 12.45, a_model: 0.298
steps: 527475, episodes: 21100, mean episode reward: -942.261, reward: -18.125, time: 5.257, TD error: 16.502, c_model: 8.427, actorQ: 12.507, a_model: 0.297
steps: 529975, episodes: 21200, mean episode reward: -1065.055, reward: -23.03, time: 5.229, TD error: 16.464, c_model: 8.408, actorQ: 12.553, a_model: 0.296
steps: 532475, episodes: 21300, mean episode reward: -908.355, reward: -16.88, time: 5.215, TD error: 16.428, c_model: 8.389, actorQ: 12.599, a_model: 0.295
steps: 534975, episodes: 21400, mean episode reward: -960.04, reward: -18.076, time: 5.251, TD error: 16.391, c_model: 8.371, actorQ: 12.641, a_model: 0.294
steps: 537475, episodes: 21500, mean episode reward: -943.909, reward: -17.695, time: 5.258, TD error: 16.356, c_model: 8.353, actorQ: 12.684, a_model: 0.293
steps: 539975, episodes: 21600, mean episode reward: -935.226, reward: -17.808, time: 5.253, TD error: 16.32, c_model: 8.334, actorQ: 12.728, a_model: 0.292
steps: 542475, episodes: 21700, mean episode reward: -941.587, reward: -18.019, time: 5.297, TD error: 16.274, c_model: 8.311, actorQ: 12.782, a_model: 0.291
steps: 544975, episodes: 21800, mean episode reward: -1001.319, reward: -20.707, time: 5.267, TD error: 16.241, c_model: 8.294, actorQ: 12.828, a_model: 0.29
steps: 547475, episodes: 21900, mean episode reward: -1002.049, reward: -20.462, time: 5.287, TD error: 16.207, c_model: 8.277, actorQ: 12.873, a_model: 0.29
steps: 549975, episodes: 22000, mean episode reward: -1021.659, reward: -20.951, time: 5.254, TD error: 16.172, c_model: 8.259, actorQ: 12.915, a_model: 0.289
steps: 552475, episodes: 22100, mean episode reward: -1012.14, reward: -20.822, time: 5.251, TD error: 16.138, c_model: 8.242, actorQ: 12.958, a_model: 0.288
steps: 554975, episodes: 22200, mean episode reward: -1023.39, reward: -21.479, time: 5.228, TD error: 16.103, c_model: 8.225, actorQ: 13.003, a_model: 0.287
steps: 557475, episodes: 22300, mean episode reward: -1064.117, reward: -22.094, time: 5.27, TD error: 16.058, c_model: 8.202, actorQ: 13.058, a_model: 0.286
steps: 559975, episodes: 22400, mean episode reward: -1071.802, reward: -22.936, time: 5.225, TD error: 16.023, c_model: 8.184, actorQ: 13.102, a_model: 0.285
steps: 562475, episodes: 22500, mean episode reward: -1133.727, reward: -25.914, time: 5.215, TD error: 15.991, c_model: 8.168, actorQ: 13.146, a_model: 0.284
steps: 564975, episodes: 22600, mean episode reward: -1157.662, reward: -26.007, time: 5.215, TD error: 15.96, c_model: 8.153, actorQ: 13.189, a_model: 0.283
steps: 567475, episodes: 22700, mean episode reward: -1110.878, reward: -25.533, time: 5.228, TD error: 15.925, c_model: 8.135, actorQ: 13.231, a_model: 0.283
steps: 569975, episodes: 22800, mean episode reward: -1107.111, reward: -25.32, time: 5.264, TD error: 15.891, c_model: 8.118, actorQ: 13.277, a_model: 0.282
steps: 572475, episodes: 22900, mean episode reward: -1093.557, reward: -24.469, time: 5.274, TD error: 15.851, c_model: 8.098, actorQ: 13.332, a_model: 0.281
steps: 574975, episodes: 23000, mean episode reward: -1065.905, reward: -23.26, time: 5.228, TD error: 15.816, c_model: 8.08, actorQ: 13.376, a_model: 0.28
steps: 577475, episodes: 23100, mean episode reward: -1040.072, reward: -21.318, time: 5.218, TD error: 15.783, c_model: 8.063, actorQ: 13.419, a_model: 0.279
steps: 579975, episodes: 23200, mean episode reward: -1008.149, reward: -20.334, time: 5.226, TD error: 15.752, c_model: 8.047, actorQ: 13.462, a_model: 0.278
steps: 582475, episodes: 23300, mean episode reward: -939.431, reward: -18.872, time: 5.232, TD error: 15.719, c_model: 8.031, actorQ: 13.507, a_model: 0.278
steps: 584975, episodes: 23400, mean episode reward: -954.544, reward: -18.528, time: 5.267, TD error: 15.687, c_model: 8.015, actorQ: 13.55, a_model: 0.277
steps: 587475, episodes: 23500, mean episode reward: -927.668, reward: -17.111, time: 5.307, TD error: 15.647, c_model: 7.995, actorQ: 13.605, a_model: 0.276
steps: 589975, episodes: 23600, mean episode reward: -897.063, reward: -15.863, time: 5.27, TD error: 15.617, c_model: 7.98, actorQ: 13.648, a_model: 0.275
steps: 592475, episodes: 23700, mean episode reward: -883.47, reward: -15.052, time: 5.223, TD error: 15.586, c_model: 7.964, actorQ: 13.691, a_model: 0.274
steps: 594975, episodes: 23800, mean episode reward: -858.314, reward: -14.635, time: 5.249, TD error: 15.552, c_model: 7.947, actorQ: 13.732, a_model: 0.274
steps: 597475, episodes: 23900, mean episode reward: -864.747, reward: -14.887, time: 5.338, TD error: 15.522, c_model: 7.931, actorQ: 13.779, a_model: 0.273
steps: 599975, episodes: 24000, mean episode reward: -813.357, reward: -13.775, time: 5.229, TD error: 15.488, c_model: 7.915, actorQ: 13.82, a_model: 0.272
steps: 602475, episodes: 24100, mean episode reward: -834.172, reward: -13.576, time: 5.268, TD error: 15.392, c_model: 7.866, actorQ: 13.908, a_model: 0.268
steps: 604975, episodes: 24200, mean episode reward: -813.107, reward: -12.801, time: 5.217, TD error: 15.286, c_model: 7.813, actorQ: 14.001, a_model: 0.264
steps: 607475, episodes: 24300, mean episode reward: -792.991, reward: -12.35, time: 5.26, TD error: 15.179, c_model: 7.759, actorQ: 14.097, a_model: 0.259
steps: 609975, episodes: 24400, mean episode reward: -774.751, reward: -11.691, time: 5.245, TD error: 15.074, c_model: 7.706, actorQ: 14.19, a_model: 0.254
steps: 612475, episodes: 24500, mean episode reward: -799.76, reward: -12.527, time: 5.215, TD error: 14.966, c_model: 7.652, actorQ: 14.282, a_model: 0.25
steps: 614975, episodes: 24600, mean episode reward: -791.264, reward: -12.162, time: 5.222, TD error: 14.858, c_model: 7.597, actorQ: 14.374, a_model: 0.245
steps: 617475, episodes: 24700, mean episode reward: -778.975, reward: -11.838, time: 5.275, TD error: 14.727, c_model: 7.532, actorQ: 14.493, a_model: 0.239
steps: 619975, episodes: 24800, mean episode reward: -759.678, reward: -11.386, time: 5.215, TD error: 14.622, c_model: 7.479, actorQ: 14.585, a_model: 0.235
steps: 622475, episodes: 24900, mean episode reward: -769.042, reward: -11.709, time: 5.211, TD error: 14.525, c_model: 7.429, actorQ: 14.677, a_model: 0.231
steps: 624975, episodes: 25000, mean episode reward: -741.896, reward: -10.91, time: 5.236, TD error: 14.423, c_model: 7.378, actorQ: 14.768, a_model: 0.227
steps: 627475, episodes: 25100, mean episode reward: -747.758, reward: -11.293, time: 5.245, TD error: 14.328, c_model: 7.329, actorQ: 14.86, a_model: 0.223
steps: 629975, episodes: 25200, mean episode reward: -728.503, reward: -11.253, time: 5.228, TD error: 14.236, c_model: 7.283, actorQ: 14.95, a_model: 0.219
steps: 632475, episodes: 25300, mean episode reward: -764.833, reward: -11.864, time: 5.295, TD error: 14.128, c_model: 7.228, actorQ: 15.059, a_model: 0.214
steps: 634975, episodes: 25400, mean episode reward: -775.441, reward: -12.501, time: 5.227, TD error: 14.04, c_model: 7.183, actorQ: 15.145, a_model: 0.211
steps: 637475, episodes: 25500, mean episode reward: -746.638, reward: -11.868, time: 5.586, TD error: 13.951, c_model: 7.137, actorQ: 15.232, a_model: 0.207
steps: 639975, episodes: 25600, mean episode reward: -763.992, reward: -12.265, time: 5.222, TD error: 13.866, c_model: 7.093, actorQ: 15.319, a_model: 0.204
steps: 642475, episodes: 25700, mean episode reward: -739.818, reward: -12.018, time: 5.401, TD error: 13.787, c_model: 7.052, actorQ: 15.406, a_model: 0.2
steps: 644975, episodes: 25800, mean episode reward: -760.67, reward: -12.346, time: 5.77, TD error: 13.7, c_model: 7.008, actorQ: 15.493, a_model: 0.197
steps: 647475, episodes: 25900, mean episode reward: -765.983, reward: -12.068, time: 5.565, TD error: 13.606, c_model: 6.959, actorQ: 15.6, a_model: 0.193
steps: 649975, episodes: 26000, mean episode reward: -767.612, reward: -11.936, time: 5.261, TD error: 13.542, c_model: 6.926, actorQ: 15.688, a_model: 0.191
steps: 652475, episodes: 26100, mean episode reward: -768.654, reward: -12.021, time: 5.223, TD error: 13.478, c_model: 6.892, actorQ: 15.775, a_model: 0.188
steps: 654975, episodes: 26200, mean episode reward: -740.462, reward: -11.619, time: 5.201, TD error: 13.407, c_model: 6.856, actorQ: 15.862, a_model: 0.185
steps: 657475, episodes: 26300, mean episode reward: -765.901, reward: -11.882, time: 5.234, TD error: 13.346, c_model: 6.824, actorQ: 15.949, a_model: 0.183
steps: 659975, episodes: 26400, mean episode reward: -782.124, reward: -11.759, time: 5.232, TD error: 13.29, c_model: 6.795, actorQ: 16.034, a_model: 0.18
steps: 662475, episodes: 26500, mean episode reward: -806.815, reward: -12.557, time: 5.254, TD error: 13.213, c_model: 6.755, actorQ: 16.14, a_model: 0.178
steps: 664975, episodes: 26600, mean episode reward: -789.751, reward: -11.885, time: 5.278, TD error: 13.158, c_model: 6.726, actorQ: 16.226, a_model: 0.176
steps: 667475, episodes: 26700, mean episode reward: -741.02, reward: -11.102, time: 5.235, TD error: 13.105, c_model: 6.698, actorQ: 16.312, a_model: 0.174
steps: 669975, episodes: 26800, mean episode reward: -762.79, reward: -11.291, time: 5.231, TD error: 13.052, c_model: 6.671, actorQ: 16.4, a_model: 0.172
steps: 672475, episodes: 26900, mean episode reward: -761.57, reward: -11.165, time: 5.235, TD error: 12.998, c_model: 6.642, actorQ: 16.486, a_model: 0.17
steps: 674975, episodes: 27000, mean episode reward: -759.47, reward: -11.351, time: 5.185, TD error: 12.941, c_model: 6.613, actorQ: 16.57, a_model: 0.168
steps: 677475, episodes: 27100, mean episode reward: -773.176, reward: -11.629, time: 5.258, TD error: 12.875, c_model: 6.578, actorQ: 16.675, a_model: 0.166
steps: 679975, episodes: 27200, mean episode reward: -753.137, reward: -11.393, time: 5.216, TD error: 12.823, c_model: 6.551, actorQ: 16.759, a_model: 0.164
steps: 682475, episodes: 27300, mean episode reward: -768.046, reward: -11.436, time: 5.22, TD error: 12.775, c_model: 6.526, actorQ: 16.841, a_model: 0.163
steps: 684975, episodes: 27400, mean episode reward: -745.722, reward: -10.781, time: 5.248, TD error: 12.725, c_model: 6.499, actorQ: 16.925, a_model: 0.161
steps: 687475, episodes: 27500, mean episode reward: -745.934, reward: -10.782, time: 5.263, TD error: 12.672, c_model: 6.472, actorQ: 17.009, a_model: 0.159
steps: 689975, episodes: 27600, mean episode reward: -737.614, reward: -10.877, time: 5.237, TD error: 12.624, c_model: 6.447, actorQ: 17.092, a_model: 0.158
steps: 692475, episodes: 27700, mean episode reward: -730.001, reward: -10.894, time: 5.27, TD error: 12.562, c_model: 6.414, actorQ: 17.198, a_model: 0.156
steps: 694975, episodes: 27800, mean episode reward: -752.549, reward: -10.852, time: 5.228, TD error: 12.506, c_model: 6.385, actorQ: 17.279, a_model: 0.155
steps: 697475, episodes: 27900, mean episode reward: -753.351, reward: -11.087, time: 5.237, TD error: 12.457, c_model: 6.36, actorQ: 17.361, a_model: 0.153
steps: 699975, episodes: 28000, mean episode reward: -763.177, reward: -11.475, time: 5.196, TD error: 12.411, c_model: 6.335, actorQ: 17.446, a_model: 0.152
steps: 702475, episodes: 28100, mean episode reward: -739.835, reward: -10.673, time: 5.239, TD error: 12.362, c_model: 6.309, actorQ: 17.53, a_model: 0.15
steps: 704975, episodes: 28200, mean episode reward: -717.672, reward: -10.763, time: 5.225, TD error: 12.308, c_model: 6.281, actorQ: 17.612, a_model: 0.149
steps: 707475, episodes: 28300, mean episode reward: -726.987, reward: -10.704, time: 5.238, TD error: 12.243, c_model: 6.248, actorQ: 17.709, a_model: 0.147
steps: 709975, episodes: 28400, mean episode reward: -732.79, reward: -11.128, time: 5.235, TD error: 12.193, c_model: 6.221, actorQ: 17.792, a_model: 0.146
steps: 712475, episodes: 28500, mean episode reward: -725.776, reward: -11.068, time: 5.329, TD error: 12.15, c_model: 6.199, actorQ: 17.875, a_model: 0.145
steps: 714975, episodes: 28600, mean episode reward: -748.181, reward: -11.733, time: 5.231, TD error: 12.098, c_model: 6.171, actorQ: 17.953, a_model: 0.143
steps: 717475, episodes: 28700, mean episode reward: -766.56, reward: -12.416, time: 5.247, TD error: 12.048, c_model: 6.146, actorQ: 18.037, a_model: 0.142
steps: 719975, episodes: 28800, mean episode reward: -785.133, reward: -12.992, time: 5.243, TD error: 12.0, c_model: 6.121, actorQ: 18.12, a_model: 0.141
steps: 722475, episodes: 28900, mean episode reward: -779.525, reward: -13.134, time: 5.277, TD error: 11.943, c_model: 6.09, actorQ: 18.218, a_model: 0.139
steps: 724975, episodes: 29000, mean episode reward: -800.611, reward: -13.498, time: 5.216, TD error: 11.888, c_model: 6.062, actorQ: 18.296, a_model: 0.138
steps: 727475, episodes: 29100, mean episode reward: -805.231, reward: -13.559, time: 5.216, TD error: 11.842, c_model: 6.038, actorQ: 18.377, a_model: 0.137
steps: 729975, episodes: 29200, mean episode reward: -813.147, reward: -13.639, time: 5.205, TD error: 11.8, c_model: 6.016, actorQ: 18.458, a_model: 0.136
steps: 732475, episodes: 29300, mean episode reward: -781.076, reward: -12.764, time: 5.234, TD error: 11.745, c_model: 5.987, actorQ: 18.535, a_model: 0.135
steps: 734975, episodes: 29400, mean episode reward: -761.327, reward: -12.399, time: 5.256, TD error: 11.695, c_model: 5.962, actorQ: 18.613, a_model: 0.134
steps: 737475, episodes: 29500, mean episode reward: -767.116, reward: -12.668, time: 5.262, TD error: 11.632, c_model: 5.929, actorQ: 18.711, a_model: 0.132
steps: 739975, episodes: 29600, mean episode reward: -772.671, reward: -12.917, time: 5.237, TD error: 11.582, c_model: 5.903, actorQ: 18.793, a_model: 0.131
steps: 742475, episodes: 29700, mean episode reward: -790.461, reward: -13.583, time: 5.227, TD error: 11.54, c_model: 5.881, actorQ: 18.873, a_model: 0.13
steps: 744975, episodes: 29800, mean episode reward: -753.524, reward: -12.432, time: 5.241, TD error: 11.5, c_model: 5.86, actorQ: 18.953, a_model: 0.129
steps: 747475, episodes: 29900, mean episode reward: -728.061, reward: -11.944, time: 5.211, TD error: 11.457, c_model: 5.838, actorQ: 19.031, a_model: 0.128
steps: 749975, episodes: 30000, mean episode reward: -738.399, reward: -11.869, time: 5.223, TD error: 11.414, c_model: 5.816, actorQ: 19.109, a_model: 0.127
steps: 752475, episodes: 30100, mean episode reward: -751.352, reward: -12.076, time: 5.28, TD error: 11.361, c_model: 5.788, actorQ: 19.204, a_model: 0.126
steps: 754975, episodes: 30200, mean episode reward: -732.046, reward: -12.036, time: 5.242, TD error: 11.318, c_model: 5.765, actorQ: 19.282, a_model: 0.125
steps: 757475, episodes: 30300, mean episode reward: -734.187, reward: -12.074, time: 5.218, TD error: 11.277, c_model: 5.745, actorQ: 19.363, a_model: 0.124
steps: 759975, episodes: 30400, mean episode reward: -757.919, reward: -12.162, time: 5.274, TD error: 11.233, c_model: 5.722, actorQ: 19.44, a_model: 0.123
steps: 762475, episodes: 30500, mean episode reward: -745.545, reward: -12.054, time: 5.239, TD error: 11.191, c_model: 5.7, actorQ: 19.514, a_model: 0.122
steps: 764975, episodes: 30600, mean episode reward: -762.682, reward: -12.426, time: 5.195, TD error: 11.145, c_model: 5.676, actorQ: 19.59, a_model: 0.121
steps: 767475, episodes: 30700, mean episode reward: -763.161, reward: -12.435, time: 5.289, TD error: 11.094, c_model: 5.65, actorQ: 19.681, a_model: 0.12
steps: 769975, episodes: 30800, mean episode reward: -746.614, reward: -12.21, time: 5.264, TD error: 11.055, c_model: 5.63, actorQ: 19.755, a_model: 0.119
steps: 772475, episodes: 30900, mean episode reward: -745.075, reward: -12.408, time: 5.238, TD error: 11.01, c_model: 5.607, actorQ: 19.829, a_model: 0.118
steps: 774975, episodes: 31000, mean episode reward: -752.002, reward: -12.171, time: 5.215, TD error: 10.971, c_model: 5.587, actorQ: 19.906, a_model: 0.118
steps: 777475, episodes: 31100, mean episode reward: -722.222, reward: -11.816, time: 5.67, TD error: 10.933, c_model: 5.568, actorQ: 19.98, a_model: 0.117
steps: 779975, episodes: 31200, mean episode reward: -722.398, reward: -11.375, time: 5.243, TD error: 10.896, c_model: 5.549, actorQ: 20.055, a_model: 0.116
steps: 782475, episodes: 31300, mean episode reward: -745.827, reward: -11.908, time: 5.244, TD error: 10.849, c_model: 5.525, actorQ: 20.15, a_model: 0.115
steps: 784975, episodes: 31400, mean episode reward: -737.364, reward: -11.599, time: 5.205, TD error: 10.811, c_model: 5.506, actorQ: 20.225, a_model: 0.114
steps: 787475, episodes: 31500, mean episode reward: -750.171, reward: -12.221, time: 5.249, TD error: 10.77, c_model: 5.485, actorQ: 20.299, a_model: 0.114
steps: 789975, episodes: 31600, mean episode reward: -741.884, reward: -12.125, time: 5.466, TD error: 10.732, c_model: 5.466, actorQ: 20.371, a_model: 0.113
steps: 792475, episodes: 31700, mean episode reward: -741.476, reward: -11.928, time: 5.848, TD error: 10.699, c_model: 5.449, actorQ: 20.443, a_model: 0.112
steps: 794975, episodes: 31800, mean episode reward: -757.085, reward: -12.128, time: 5.839, TD error: 10.659, c_model: 5.429, actorQ: 20.511, a_model: 0.112
steps: 797475, episodes: 31900, mean episode reward: -755.679, reward: -12.275, time: 5.361, TD error: 10.616, c_model: 5.407, actorQ: 20.601, a_model: 0.111
steps: 799975, episodes: 32000, mean episode reward: -757.678, reward: -12.301, time: 5.259, TD error: 10.581, c_model: 5.39, actorQ: 20.672, a_model: 0.11
steps: 802475, episodes: 32100, mean episode reward: -753.393, reward: -12.023, time: 5.263, TD error: 10.541, c_model: 5.37, actorQ: 20.742, a_model: 0.11
steps: 804975, episodes: 32200, mean episode reward: -762.44, reward: -12.447, time: 5.239, TD error: 10.501, c_model: 5.35, actorQ: 20.813, a_model: 0.109
steps: 807475, episodes: 32300, mean episode reward: -750.165, reward: -12.029, time: 5.266, TD error: 10.469, c_model: 5.334, actorQ: 20.883, a_model: 0.108
steps: 809975, episodes: 32400, mean episode reward: -789.472, reward: -13.0, time: 5.235, TD error: 10.434, c_model: 5.316, actorQ: 20.952, a_model: 0.108
steps: 812475, episodes: 32500, mean episode reward: -753.471, reward: -12.182, time: 5.302, TD error: 10.395, c_model: 5.297, actorQ: 21.042, a_model: 0.107
steps: 814975, episodes: 32600, mean episode reward: -762.277, reward: -12.328, time: 5.24, TD error: 10.36, c_model: 5.279, actorQ: 21.114, a_model: 0.107
steps: 817475, episodes: 32700, mean episode reward: -748.789, reward: -12.1, time: 5.273, TD error: 10.329, c_model: 5.264, actorQ: 21.182, a_model: 0.106
steps: 819975, episodes: 32800, mean episode reward: -775.44, reward: -12.56, time: 5.268, TD error: 10.303, c_model: 5.251, actorQ: 21.254, a_model: 0.106
steps: 822475, episodes: 32900, mean episode reward: -755.455, reward: -12.494, time: 5.293, TD error: 10.267, c_model: 5.233, actorQ: 21.322, a_model: 0.105
steps: 824975, episodes: 33000, mean episode reward: -780.341, reward: -12.535, time: 5.236, TD error: 10.232, c_model: 5.216, actorQ: 21.39, a_model: 0.105
steps: 827475, episodes: 33100, mean episode reward: -789.361, reward: -12.778, time: 5.272, TD error: 10.195, c_model: 5.198, actorQ: 21.476, a_model: 0.104
steps: 829975, episodes: 33200, mean episode reward: -803.441, reward: -12.807, time: 5.227, TD error: 10.16, c_model: 5.181, actorQ: 21.545, a_model: 0.104
steps: 832475, episodes: 33300, mean episode reward: -799.314, reward: -13.009, time: 5.249, TD error: 10.13, c_model: 5.166, actorQ: 21.613, a_model: 0.103
steps: 834975, episodes: 33400, mean episode reward: -803.292, reward: -12.889, time: 5.245, TD error: 10.094, c_model: 5.148, actorQ: 21.681, a_model: 0.103
steps: 837475, episodes: 33500, mean episode reward: -790.32, reward: -12.902, time: 5.258, TD error: 10.062, c_model: 5.133, actorQ: 21.745, a_model: 0.102
steps: 839975, episodes: 33600, mean episode reward: -751.024, reward: -12.22, time: 5.226, TD error: 10.029, c_model: 5.117, actorQ: 21.81, a_model: 0.102
steps: 842475, episodes: 33700, mean episode reward: -753.091, reward: -12.26, time: 5.276, TD error: 9.993, c_model: 5.099, actorQ: 21.891, a_model: 0.101
steps: 844975, episodes: 33800, mean episode reward: -715.873, reward: -11.827, time: 5.263, TD error: 9.961, c_model: 5.083, actorQ: 21.957, a_model: 0.101
steps: 847475, episodes: 33900, mean episode reward: -717.968, reward: -11.475, time: 5.27, TD error: 9.93, c_model: 5.069, actorQ: 22.022, a_model: 0.101
steps: 849975, episodes: 34000, mean episode reward: -714.281, reward: -11.259, time: 5.253, TD error: 9.898, c_model: 5.053, actorQ: 22.085, a_model: 0.1
steps: 852475, episodes: 34100, mean episode reward: -701.516, reward: -10.684, time: 5.257, TD error: 9.871, c_model: 5.04, actorQ: 22.147, a_model: 0.1
steps: 854975, episodes: 34200, mean episode reward: -695.019, reward: -10.622, time: 5.295, TD error: 9.842, c_model: 5.025, actorQ: 22.211, a_model: 0.099
steps: 857475, episodes: 34300, mean episode reward: -695.59, reward: -10.819, time: 5.287, TD error: 9.8, c_model: 5.005, actorQ: 22.285, a_model: 0.099
steps: 859975, episodes: 34400, mean episode reward: -703.524, reward: -10.923, time: 5.25, TD error: 9.765, c_model: 4.988, actorQ: 22.346, a_model: 0.099
steps: 862475, episodes: 34500, mean episode reward: -693.607, reward: -10.159, time: 5.283, TD error: 9.73, c_model: 4.971, actorQ: 22.406, a_model: 0.098
steps: 864975, episodes: 34600, mean episode reward: -688.626, reward: -10.158, time: 5.329, TD error: 9.697, c_model: 4.956, actorQ: 22.469, a_model: 0.098
steps: 867475, episodes: 34700, mean episode reward: -698.014, reward: -10.329, time: 5.211, TD error: 9.667, c_model: 4.942, actorQ: 22.533, a_model: 0.098
steps: 869975, episodes: 34800, mean episode reward: -665.975, reward: -9.405, time: 5.266, TD error: 9.636, c_model: 4.927, actorQ: 22.592, a_model: 0.097
steps: 872475, episodes: 34900, mean episode reward: -672.937, reward: -9.518, time: 5.274, TD error: 9.602, c_model: 4.911, actorQ: 22.668, a_model: 0.097
steps: 874975, episodes: 35000, mean episode reward: -669.066, reward: -9.457, time: 5.244, TD error: 9.565, c_model: 4.893, actorQ: 22.727, a_model: 0.097
steps: 877475, episodes: 35100, mean episode reward: -649.088, reward: -9.338, time: 5.258, TD error: 9.534, c_model: 4.878, actorQ: 22.788, a_model: 0.096
steps: 879975, episodes: 35200, mean episode reward: -664.179, reward: -9.359, time: 5.233, TD error: 9.502, c_model: 4.863, actorQ: 22.85, a_model: 0.096
steps: 882475, episodes: 35300, mean episode reward: -658.118, reward: -9.307, time: 5.334, TD error: 9.469, c_model: 4.847, actorQ: 22.914, a_model: 0.096
steps: 884975, episodes: 35400, mean episode reward: -656.118, reward: -9.584, time: 5.242, TD error: 9.44, c_model: 4.834, actorQ: 22.976, a_model: 0.095
steps: 887475, episodes: 35500, mean episode reward: -703.18, reward: -10.467, time: 5.285, TD error: 9.399, c_model: 4.814, actorQ: 23.052, a_model: 0.095
steps: 889975, episodes: 35600, mean episode reward: -670.18, reward: -9.799, time: 5.267, TD error: 9.374, c_model: 4.802, actorQ: 23.113, a_model: 0.095
steps: 892475, episodes: 35700, mean episode reward: -697.661, reward: -10.341, time: 5.255, TD error: 9.343, c_model: 4.787, actorQ: 23.17, a_model: 0.094
steps: 894975, episodes: 35800, mean episode reward: -699.251, reward: -10.516, time: 5.23, TD error: 9.313, c_model: 4.772, actorQ: 23.227, a_model: 0.094
steps: 897475, episodes: 35900, mean episode reward: -700.284, reward: -10.421, time: 5.229, TD error: 9.288, c_model: 4.761, actorQ: 23.287, a_model: 0.094
steps: 899975, episodes: 36000, mean episode reward: -708.188, reward: -10.676, time: 5.22, TD error: 9.263, c_model: 4.749, actorQ: 23.346, a_model: 0.093
steps: 902475, episodes: 36100, mean episode reward: -723.728, reward: -10.775, time: 5.292, TD error: 9.224, c_model: 4.731, actorQ: 23.416, a_model: 0.093
steps: 904975, episodes: 36200, mean episode reward: -683.953, reward: -10.24, time: 5.254, TD error: 9.192, c_model: 4.715, actorQ: 23.471, a_model: 0.093
steps: 907475, episodes: 36300, mean episode reward: -664.69, reward: -9.763, time: 5.225, TD error: 9.164, c_model: 4.701, actorQ: 23.528, a_model: 0.093
steps: 909975, episodes: 36400, mean episode reward: -684.142, reward: -10.113, time: 5.24, TD error: 9.135, c_model: 4.688, actorQ: 23.583, a_model: 0.092
steps: 912475, episodes: 36500, mean episode reward: -684.362, reward: -9.996, time: 5.327, TD error: 9.104, c_model: 4.673, actorQ: 23.637, a_model: 0.092
steps: 914975, episodes: 36600, mean episode reward: -665.078, reward: -9.761, time: 5.263, TD error: 9.071, c_model: 4.656, actorQ: 23.688, a_model: 0.092
steps: 917475, episodes: 36700, mean episode reward: -677.246, reward: -9.755, time: 5.279, TD error: 9.038, c_model: 4.641, actorQ: 23.759, a_model: 0.091
steps: 919975, episodes: 36800, mean episode reward: -666.388, reward: -9.664, time: 5.226, TD error: 9.007, c_model: 4.625, actorQ: 23.814, a_model: 0.091
steps: 922475, episodes: 36900, mean episode reward: -705.313, reward: -10.324, time: 5.228, TD error: 8.976, c_model: 4.611, actorQ: 23.868, a_model: 0.091
steps: 924975, episodes: 37000, mean episode reward: -676.898, reward: -9.888, time: 5.217, TD error: 8.947, c_model: 4.597, actorQ: 23.92, a_model: 0.091
steps: 927475, episodes: 37100, mean episode reward: -688.902, reward: -9.878, time: 5.262, TD error: 8.914, c_model: 4.581, actorQ: 23.975, a_model: 0.091
steps: 929975, episodes: 37200, mean episode reward: -673.369, reward: -9.735, time: 5.244, TD error: 8.887, c_model: 4.568, actorQ: 24.026, a_model: 0.09
steps: 932475, episodes: 37300, mean episode reward: -711.154, reward: -10.46, time: 5.282, TD error: 8.851, c_model: 4.551, actorQ: 24.091, a_model: 0.09
steps: 934975, episodes: 37400, mean episode reward: -660.628, reward: -9.56, time: 5.218, TD error: 8.82, c_model: 4.536, actorQ: 24.145, a_model: 0.09
steps: 937475, episodes: 37500, mean episode reward: -684.614, reward: -10.03, time: 5.607, TD error: 8.794, c_model: 4.524, actorQ: 24.198, a_model: 0.089
steps: 939975, episodes: 37600, mean episode reward: -674.027, reward: -9.817, time: 5.747, TD error: 8.766, c_model: 4.51, actorQ: 24.253, a_model: 0.089
steps: 942475, episodes: 37700, mean episode reward: -689.276, reward: -9.844, time: 5.229, TD error: 8.736, c_model: 4.496, actorQ: 24.304, a_model: 0.089
steps: 944975, episodes: 37800, mean episode reward: -663.521, reward: -9.434, time: 5.219, TD error: 8.712, c_model: 4.484, actorQ: 24.353, a_model: 0.089
steps: 947475, episodes: 37900, mean episode reward: -653.961, reward: -9.256, time: 5.304, TD error: 8.681, c_model: 4.468, actorQ: 24.416, a_model: 0.089
steps: 949975, episodes: 38000, mean episode reward: -665.059, reward: -9.411, time: 5.277, TD error: 8.651, c_model: 4.454, actorQ: 24.474, a_model: 0.088
steps: 952475, episodes: 38100, mean episode reward: -702.291, reward: -10.04, time: 5.228, TD error: 8.632, c_model: 4.445, actorQ: 24.534, a_model: 0.088
steps: 954975, episodes: 38200, mean episode reward: -681.043, reward: -9.704, time: 5.241, TD error: 8.61, c_model: 4.435, actorQ: 24.59, a_model: 0.088
steps: 957475, episodes: 38300, mean episode reward: -650.48, reward: -9.428, time: 5.192, TD error: 8.59, c_model: 4.426, actorQ: 24.645, a_model: 0.088
steps: 959975, episodes: 38400, mean episode reward: -646.898, reward: -9.174, time: 5.272, TD error: 8.569, c_model: 4.416, actorQ: 24.702, a_model: 0.087
steps: 962475, episodes: 38500, mean episode reward: -684.321, reward: -9.708, time: 5.282, TD error: 8.547, c_model: 4.406, actorQ: 24.774, a_model: 0.087
steps: 964975, episodes: 38600, mean episode reward: -706.272, reward: -10.14, time: 5.235, TD error: 8.525, c_model: 4.396, actorQ: 24.828, a_model: 0.087
steps: 967475, episodes: 38700, mean episode reward: -671.339, reward: -9.523, time: 5.223, TD error: 8.507, c_model: 4.388, actorQ: 24.881, a_model: 0.087
steps: 969975, episodes: 38800, mean episode reward: -672.716, reward: -9.807, time: 5.238, TD error: 8.494, c_model: 4.382, actorQ: 24.934, a_model: 0.087
steps: 972475, episodes: 38900, mean episode reward: -650.488, reward: -9.461, time: 5.77, TD error: 8.475, c_model: 4.373, actorQ: 24.986, a_model: 0.086
steps: 974975, episodes: 39000, mean episode reward: -668.432, reward: -9.695, time: 5.254, TD error: 8.456, c_model: 4.364, actorQ: 25.04, a_model: 0.086
steps: 977475, episodes: 39100, mean episode reward: -650.265, reward: -9.395, time: 5.282, TD error: 8.436, c_model: 4.355, actorQ: 25.107, a_model: 0.086
steps: 979975, episodes: 39200, mean episode reward: -665.809, reward: -9.653, time: 5.238, TD error: 8.424, c_model: 4.349, actorQ: 25.16, a_model: 0.086
steps: 982475, episodes: 39300, mean episode reward: -675.441, reward: -9.825, time: 5.238, TD error: 8.413, c_model: 4.344, actorQ: 25.212, a_model: 0.085
steps: 984975, episodes: 39400, mean episode reward: -685.755, reward: -10.084, time: 5.228, TD error: 8.398, c_model: 4.337, actorQ: 25.262, a_model: 0.085
steps: 987475, episodes: 39500, mean episode reward: -718.2, reward: -10.669, time: 5.246, TD error: 8.381, c_model: 4.329, actorQ: 25.308, a_model: 0.085
steps: 989975, episodes: 39600, mean episode reward: -719.725, reward: -10.692, time: 5.296, TD error: 8.372, c_model: 4.325, actorQ: 25.361, a_model: 0.085
steps: 992475, episodes: 39700, mean episode reward: -688.42, reward: -10.283, time: 5.559, TD error: 8.358, c_model: 4.318, actorQ: 25.418, a_model: 0.085
steps: 994975, episodes: 39800, mean episode reward: -684.517, reward: -10.316, time: 5.32, TD error: 8.344, c_model: 4.312, actorQ: 25.468, a_model: 0.084
steps: 997475, episodes: 39900, mean episode reward: -670.697, reward: -9.864, time: 5.252, TD error: 8.333, c_model: 4.307, actorQ: 25.517, a_model: 0.084
steps: 999975, episodes: 40000, mean episode reward: -683.662, reward: -9.921, time: 5.213, TD error: 8.325, c_model: 4.303, actorQ: 25.564, a_model: 0.084
steps: 1002475, episodes: 40100, mean episode reward: -675.417, reward: -10.009, time: 5.278, TD error: 8.312, c_model: 4.297, actorQ: 25.609, a_model: 0.084
steps: 1004975, episodes: 40200, mean episode reward: -659.487, reward: -9.768, time: 5.226, TD error: 8.301, c_model: 4.292, actorQ: 25.657, a_model: 0.084
steps: 1007475, episodes: 40300, mean episode reward: -657.925, reward: -9.786, time: 5.287, TD error: 8.29, c_model: 4.287, actorQ: 25.713, a_model: 0.083
steps: 1009975, episodes: 40400, mean episode reward: -679.738, reward: -9.996, time: 5.217, TD error: 8.283, c_model: 4.283, actorQ: 25.762, a_model: 0.083
steps: 1012475, episodes: 40500, mean episode reward: -665.3, reward: -9.618, time: 5.249, TD error: 8.272, c_model: 4.278, actorQ: 25.808, a_model: 0.083
steps: 1014975, episodes: 40600, mean episode reward: -692.839, reward: -10.447, time: 5.242, TD error: 8.261, c_model: 4.273, actorQ: 25.854, a_model: 0.083
steps: 1017475, episodes: 40700, mean episode reward: -649.509, reward: -9.255, time: 5.219, TD error: 8.252, c_model: 4.269, actorQ: 25.899, a_model: 0.083
steps: 1019975, episodes: 40800, mean episode reward: -689.881, reward: -10.066, time: 5.238, TD error: 8.245, c_model: 4.266, actorQ: 25.946, a_model: 0.082
steps: 1022475, episodes: 40900, mean episode reward: -667.696, reward: -9.87, time: 5.291, TD error: 8.234, c_model: 4.261, actorQ: 26.002, a_model: 0.082
steps: 1024975, episodes: 41000, mean episode reward: -697.489, reward: -10.419, time: 5.25, TD error: 8.224, c_model: 4.256, actorQ: 26.048, a_model: 0.082
steps: 1027475, episodes: 41100, mean episode reward: -695.111, reward: -10.307, time: 5.26, TD error: 8.215, c_model: 4.252, actorQ: 26.093, a_model: 0.082
steps: 1029975, episodes: 41200, mean episode reward: -690.33, reward: -10.229, time: 5.219, TD error: 8.207, c_model: 4.249, actorQ: 26.135, a_model: 0.082
steps: 1032475, episodes: 41300, mean episode reward: -657.766, reward: -9.612, time: 5.259, TD error: 8.201, c_model: 4.245, actorQ: 26.176, a_model: 0.082
steps: 1034975, episodes: 41400, mean episode reward: -677.404, reward: -10.233, time: 5.218, TD error: 8.196, c_model: 4.243, actorQ: 26.222, a_model: 0.082
steps: 1037475, episodes: 41500, mean episode reward: -669.227, reward: -9.865, time: 5.303, TD error: 8.191, c_model: 4.241, actorQ: 26.276, a_model: 0.081
steps: 1039975, episodes: 41600, mean episode reward: -683.137, reward: -10.293, time: 5.219, TD error: 8.183, c_model: 4.237, actorQ: 26.317, a_model: 0.081
steps: 1042475, episodes: 41700, mean episode reward: -704.399, reward: -10.417, time: 5.241, TD error: 8.178, c_model: 4.235, actorQ: 26.359, a_model: 0.081
steps: 1044975, episodes: 41800, mean episode reward: -687.571, reward: -10.361, time: 5.244, TD error: 8.171, c_model: 4.232, actorQ: 26.401, a_model: 0.081
steps: 1047475, episodes: 41900, mean episode reward: -660.355, reward: -9.83, time: 5.228, TD error: 8.164, c_model: 4.228, actorQ: 26.441, a_model: 0.081
steps: 1049975, episodes: 42000, mean episode reward: -677.838, reward: -9.852, time: 5.248, TD error: 8.161, c_model: 4.227, actorQ: 26.482, a_model: 0.08
steps: 1052475, episodes: 42100, mean episode reward: -665.656, reward: -9.674, time: 5.341, TD error: 8.155, c_model: 4.224, actorQ: 26.531, a_model: 0.08
steps: 1054975, episodes: 42200, mean episode reward: -647.379, reward: -9.262, time: 5.317, TD error: 8.15, c_model: 4.221, actorQ: 26.571, a_model: 0.08
steps: 1057475, episodes: 42300, mean episode reward: -652.465, reward: -9.475, time: 5.283, TD error: 8.147, c_model: 4.22, actorQ: 26.613, a_model: 0.08
steps: 1059975, episodes: 42400, mean episode reward: -652.001, reward: -9.269, time: 5.172, TD error: 8.146, c_model: 4.219, actorQ: 26.655, a_model: 0.08
steps: 1062475, episodes: 42500, mean episode reward: -635.141, reward: -9.098, time: 5.193, TD error: 8.141, c_model: 4.217, actorQ: 26.693, a_model: 0.079
steps: 1064975, episodes: 42600, mean episode reward: -657.66, reward: -9.596, time: 5.217, TD error: 8.141, c_model: 4.217, actorQ: 26.732, a_model: 0.079
steps: 1067475, episodes: 42700, mean episode reward: -643.262, reward: -9.596, time: 5.26, TD error: 8.137, c_model: 4.216, actorQ: 26.783, a_model: 0.079
steps: 1069975, episodes: 42800, mean episode reward: -649.543, reward: -9.398, time: 5.243, TD error: 8.132, c_model: 4.213, actorQ: 26.823, a_model: 0.079
steps: 1072475, episodes: 42900, mean episode reward: -660.564, reward: -9.517, time: 5.264, TD error: 8.13, c_model: 4.212, actorQ: 26.861, a_model: 0.079
steps: 1074975, episodes: 43000, mean episode reward: -670.392, reward: -9.752, time: 5.234, TD error: 8.129, c_model: 4.212, actorQ: 26.901, a_model: 0.079
steps: 1077475, episodes: 43100, mean episode reward: -655.797, reward: -9.42, time: 5.233, TD error: 8.123, c_model: 4.209, actorQ: 26.941, a_model: 0.078
steps: 1079975, episodes: 43200, mean episode reward: -656.077, reward: -9.341, time: 5.277, TD error: 8.122, c_model: 4.209, actorQ: 26.984, a_model: 0.078
steps: 1082475, episodes: 43300, mean episode reward: -644.993, reward: -9.392, time: 5.306, TD error: 8.118, c_model: 4.207, actorQ: 27.036, a_model: 0.078
steps: 1084975, episodes: 43400, mean episode reward: -683.42, reward: -10.114, time: 5.855, TD error: 8.113, c_model: 4.204, actorQ: 27.072, a_model: 0.078
steps: 1087475, episodes: 43500, mean episode reward: -681.852, reward: -10.037, time: 5.575, TD error: 8.108, c_model: 4.202, actorQ: 27.11, a_model: 0.078
steps: 1089975, episodes: 43600, mean episode reward: -671.05, reward: -9.691, time: 5.234, TD error: 8.102, c_model: 4.199, actorQ: 27.146, a_model: 0.078
steps: 1092475, episodes: 43700, mean episode reward: -673.636, reward: -9.91, time: 5.254, TD error: 8.098, c_model: 4.197, actorQ: 27.181, a_model: 0.078
steps: 1094975, episodes: 43800, mean episode reward: -688.895, reward: -10.306, time: 5.221, TD error: 8.092, c_model: 4.195, actorQ: 27.213, a_model: 0.077
steps: 1097475, episodes: 43900, mean episode reward: -681.016, reward: -9.971, time: 5.282, TD error: 8.089, c_model: 4.194, actorQ: 27.263, a_model: 0.077
steps: 1099975, episodes: 44000, mean episode reward: -681.821, reward: -9.989, time: 5.187, TD error: 8.085, c_model: 4.192, actorQ: 27.299, a_model: 0.077
steps: 1102475, episodes: 44100, mean episode reward: -661.578, reward: -9.667, time: 5.231, TD error: 8.079, c_model: 4.189, actorQ: 27.333, a_model: 0.077
steps: 1104975, episodes: 44200, mean episode reward: -666.621, reward: -9.805, time: 5.246, TD error: 8.075, c_model: 4.187, actorQ: 27.371, a_model: 0.077
steps: 1107475, episodes: 44300, mean episode reward: -680.071, reward: -9.675, time: 5.209, TD error: 8.072, c_model: 4.186, actorQ: 27.408, a_model: 0.077
steps: 1109975, episodes: 44400, mean episode reward: -678.557, reward: -9.608, time: 5.493, TD error: 8.075, c_model: 4.187, actorQ: 27.444, a_model: 0.076
steps: 1112475, episodes: 44500, mean episode reward: -666.487, reward: -9.504, time: 5.276, TD error: 8.074, c_model: 4.187, actorQ: 27.492, a_model: 0.076
steps: 1114975, episodes: 44600, mean episode reward: -659.116, reward: -9.704, time: 5.213, TD error: 8.067, c_model: 4.184, actorQ: 27.527, a_model: 0.076
steps: 1117475, episodes: 44700, mean episode reward: -663.374, reward: -9.716, time: 5.22, TD error: 8.063, c_model: 4.181, actorQ: 27.562, a_model: 0.076
steps: 1119975, episodes: 44800, mean episode reward: -667.455, reward: -9.659, time: 5.234, TD error: 8.059, c_model: 4.179, actorQ: 27.598, a_model: 0.076
steps: 1122475, episodes: 44900, mean episode reward: -655.176, reward: -9.512, time: 5.277, TD error: 8.056, c_model: 4.178, actorQ: 27.637, a_model: 0.076
steps: 1124975, episodes: 45000, mean episode reward: -650.43, reward: -9.447, time: 5.251, TD error: 8.053, c_model: 4.177, actorQ: 27.672, a_model: 0.076
steps: 1127475, episodes: 45100, mean episode reward: -653.674, reward: -9.376, time: 5.277, TD error: 8.05, c_model: 4.175, actorQ: 27.72, a_model: 0.075
steps: 1129975, episodes: 45200, mean episode reward: -658.325, reward: -9.587, time: 5.247, TD error: 8.048, c_model: 4.174, actorQ: 27.754, a_model: 0.075
steps: 1132475, episodes: 45300, mean episode reward: -650.065, reward: -9.367, time: 5.255, TD error: 8.044, c_model: 4.172, actorQ: 27.787, a_model: 0.075
steps: 1134975, episodes: 45400, mean episode reward: -649.887, reward: -9.188, time: 5.287, TD error: 8.041, c_model: 4.171, actorQ: 27.825, a_model: 0.075
steps: 1137475, episodes: 45500, mean episode reward: -666.936, reward: -9.582, time: 5.311, TD error: 8.041, c_model: 4.171, actorQ: 27.861, a_model: 0.075
steps: 1139975, episodes: 45600, mean episode reward: -633.368, reward: -9.094, time: 5.308, TD error: 8.035, c_model: 4.168, actorQ: 27.893, a_model: 0.075
steps: 1142475, episodes: 45700, mean episode reward: -673.058, reward: -9.597, time: 5.32, TD error: 8.034, c_model: 4.168, actorQ: 27.942, a_model: 0.075
steps: 1144975, episodes: 45800, mean episode reward: -662.898, reward: -9.556, time: 5.261, TD error: 8.032, c_model: 4.167, actorQ: 27.979, a_model: 0.075
steps: 1147475, episodes: 45900, mean episode reward: -684.563, reward: -10.194, time: 5.294, TD error: 8.027, c_model: 4.164, actorQ: 28.01, a_model: 0.074
steps: 1149975, episodes: 46000, mean episode reward: -675.714, reward: -10.085, time: 5.231, TD error: 8.024, c_model: 4.163, actorQ: 28.047, a_model: 0.074
steps: 1152475, episodes: 46100, mean episode reward: -676.543, reward: -10.161, time: 5.212, TD error: 8.021, c_model: 4.161, actorQ: 28.083, a_model: 0.074
steps: 1154975, episodes: 46200, mean episode reward: -698.016, reward: -10.788, time: 5.23, TD error: 8.018, c_model: 4.16, actorQ: 28.116, a_model: 0.074
steps: 1157475, episodes: 46300, mean episode reward: -662.461, reward: -10.105, time: 5.3, TD error: 8.021, c_model: 4.161, actorQ: 28.158, a_model: 0.074
steps: 1159975, episodes: 46400, mean episode reward: -667.117, reward: -9.835, time: 5.271, TD error: 8.018, c_model: 4.159, actorQ: 28.187, a_model: 0.074
steps: 1162475, episodes: 46500, mean episode reward: -652.644, reward: -9.622, time: 5.211, TD error: 8.016, c_model: 4.158, actorQ: 28.223, a_model: 0.074
steps: 1164975, episodes: 46600, mean episode reward: -665.352, reward: -9.64, time: 5.247, TD error: 8.011, c_model: 4.155, actorQ: 28.258, a_model: 0.074
steps: 1167475, episodes: 46700, mean episode reward: -672.001, reward: -9.724, time: 5.341, TD error: 8.009, c_model: 4.154, actorQ: 28.294, a_model: 0.073
steps: 1169975, episodes: 46800, mean episode reward: -653.538, reward: -9.693, time: 5.261, TD error: 8.005, c_model: 4.152, actorQ: 28.323, a_model: 0.073
steps: 1172475, episodes: 46900, mean episode reward: -655.386, reward: -9.522, time: 5.532, TD error: 8.002, c_model: 4.15, actorQ: 28.362, a_model: 0.073
steps: 1174975, episodes: 47000, mean episode reward: -670.281, reward: -9.823, time: 5.258, TD error: 8.001, c_model: 4.15, actorQ: 28.391, a_model: 0.073
steps: 1177475, episodes: 47100, mean episode reward: -663.595, reward: -9.839, time: 5.266, TD error: 8.001, c_model: 4.149, actorQ: 28.424, a_model: 0.073
steps: 1179975, episodes: 47200, mean episode reward: -650.566, reward: -9.388, time: 5.227, TD error: 7.997, c_model: 4.147, actorQ: 28.457, a_model: 0.073
steps: 1182475, episodes: 47300, mean episode reward: -670.507, reward: -9.936, time: 5.291, TD error: 7.992, c_model: 4.145, actorQ: 28.486, a_model: 0.073
steps: 1184975, episodes: 47400, mean episode reward: -683.603, reward: -10.47, time: 5.241, TD error: 7.995, c_model: 4.146, actorQ: 28.519, a_model: 0.073
steps: 1187475, episodes: 47500, mean episode reward: -674.745, reward: -10.178, time: 5.299, TD error: 7.991, c_model: 4.145, actorQ: 28.555, a_model: 0.073
steps: 1189975, episodes: 47600, mean episode reward: -661.729, reward: -10.053, time: 5.266, TD error: 7.988, c_model: 4.143, actorQ: 28.585, a_model: 0.072
steps: 1192475, episodes: 47700, mean episode reward: -664.855, reward: -10.015, time: 5.236, TD error: 7.985, c_model: 4.142, actorQ: 28.618, a_model: 0.072
steps: 1194975, episodes: 47800, mean episode reward: -659.697, reward: -10.018, time: 5.288, TD error: 7.989, c_model: 4.144, actorQ: 28.649, a_model: 0.072
steps: 1197475, episodes: 47900, mean episode reward: -667.646, reward: -10.153, time: 5.265, TD error: 7.985, c_model: 4.141, actorQ: 28.676, a_model: 0.072
steps: 1199975, episodes: 48000, mean episode reward: -657.319, reward: -9.637, time: 5.303, TD error: 7.985, c_model: 4.141, actorQ: 28.706, a_model: 0.072
steps: 1202475, episodes: 48100, mean episode reward: -660.375, reward: -9.835, time: 5.28, TD error: 7.985, c_model: 4.141, actorQ: 28.754, a_model: 0.072
steps: 1204975, episodes: 48200, mean episode reward: -661.618, reward: -9.833, time: 5.296, TD error: 7.983, c_model: 4.14, actorQ: 28.784, a_model: 0.072
steps: 1207475, episodes: 48300, mean episode reward: -640.646, reward: -9.56, time: 5.255, TD error: 7.981, c_model: 4.138, actorQ: 28.811, a_model: 0.072
steps: 1209975, episodes: 48400, mean episode reward: -638.985, reward: -9.188, time: 5.228, TD error: 7.978, c_model: 4.136, actorQ: 28.84, a_model: 0.072
steps: 1212475, episodes: 48500, mean episode reward: -666.136, reward: -9.822, time: 5.239, TD error: 7.974, c_model: 4.134, actorQ: 28.869, a_model: 0.072
steps: 1214975, episodes: 48600, mean episode reward: -654.586, reward: -9.585, time: 5.964, TD error: 7.971, c_model: 4.133, actorQ: 28.899, a_model: 0.072
steps: 1217475, episodes: 48700, mean episode reward: -654.164, reward: -9.363, time: 5.282, TD error: 7.964, c_model: 4.129, actorQ: 28.931, a_model: 0.072
steps: 1219975, episodes: 48800, mean episode reward: -666.531, reward: -9.616, time: 5.262, TD error: 7.964, c_model: 4.128, actorQ: 28.96, a_model: 0.071
steps: 1222475, episodes: 48900, mean episode reward: -656.086, reward: -9.776, time: 5.262, TD error: 7.961, c_model: 4.127, actorQ: 28.989, a_model: 0.071
steps: 1224975, episodes: 49000, mean episode reward: -657.946, reward: -9.542, time: 5.257, TD error: 7.957, c_model: 4.124, actorQ: 29.015, a_model: 0.071
steps: 1227475, episodes: 49100, mean episode reward: -661.753, reward: -9.759, time: 5.282, TD error: 7.95, c_model: 4.121, actorQ: 29.041, a_model: 0.071
steps: 1229975, episodes: 49200, mean episode reward: -646.239, reward: -9.511, time: 5.575, TD error: 7.946, c_model: 4.118, actorQ: 29.07, a_model: 0.071
steps: 1232475, episodes: 49300, mean episode reward: -652.707, reward: -9.832, time: 5.919, TD error: 7.94, c_model: 4.115, actorQ: 29.109, a_model: 0.071
steps: 1234975, episodes: 49400, mean episode reward: -642.642, reward: -9.393, time: 5.266, TD error: 7.939, c_model: 4.114, actorQ: 29.138, a_model: 0.071
steps: 1237475, episodes: 49500, mean episode reward: -687.648, reward: -10.232, time: 5.207, TD error: 7.935, c_model: 4.111, actorQ: 29.165, a_model: 0.071
steps: 1239975, episodes: 49600, mean episode reward: -669.248, reward: -10.041, time: 5.215, TD error: 7.929, c_model: 4.108, actorQ: 29.194, a_model: 0.071
steps: 1242475, episodes: 49700, mean episode reward: -666.894, reward: -9.997, time: 5.241, TD error: 7.925, c_model: 4.106, actorQ: 29.225, a_model: 0.071
steps: 1244975, episodes: 49800, mean episode reward: -647.038, reward: -9.567, time: 5.263, TD error: 7.925, c_model: 4.105, actorQ: 29.255, a_model: 0.071
steps: 1247475, episodes: 49900, mean episode reward: -656.398, reward: -9.794, time: 5.277, TD error: 7.923, c_model: 4.104, actorQ: 29.293, a_model: 0.071
steps: 1249975, episodes: 50000, mean episode reward: -646.013, reward: -9.41, time: 5.233, TD error: 7.917, c_model: 4.101, actorQ: 29.321, a_model: 0.07
steps: 1252475, episodes: 50100, mean episode reward: -657.875, reward: -9.58, time: 5.284, TD error: 7.916, c_model: 4.1, actorQ: 29.349, a_model: 0.07
steps: 1254975, episodes: 50200, mean episode reward: -657.265, reward: -9.701, time: 5.258, TD error: 7.913, c_model: 4.098, actorQ: 29.377, a_model: 0.07
steps: 1257475, episodes: 50300, mean episode reward: -657.804, reward: -9.749, time: 5.251, TD error: 7.909, c_model: 4.096, actorQ: 29.404, a_model: 0.07
steps: 1259975, episodes: 50400, mean episode reward: -666.043, reward: -10.0, time: 5.237, TD error: 7.904, c_model: 4.093, actorQ: 29.431, a_model: 0.07
steps: 1262475, episodes: 50500, mean episode reward: -655.065, reward: -9.753, time: 5.299, TD error: 7.901, c_model: 4.091, actorQ: 29.468, a_model: 0.07
steps: 1264975, episodes: 50600, mean episode reward: -694.549, reward: -10.602, time: 5.215, TD error: 7.895, c_model: 4.088, actorQ: 29.491, a_model: 0.07
steps: 1267475, episodes: 50700, mean episode reward: -685.446, reward: -10.368, time: 5.236, TD error: 7.889, c_model: 4.085, actorQ: 29.515, a_model: 0.07
steps: 1269975, episodes: 50800, mean episode reward: -675.579, reward: -10.044, time: 5.221, TD error: 7.883, c_model: 4.081, actorQ: 29.538, a_model: 0.07
steps: 1272475, episodes: 50900, mean episode reward: -684.217, reward: -9.893, time: 5.26, TD error: 7.878, c_model: 4.078, actorQ: 29.566, a_model: 0.07
steps: 1274975, episodes: 51000, mean episode reward: -660.697, reward: -9.555, time: 5.216, TD error: 7.874, c_model: 4.075, actorQ: 29.592, a_model: 0.07
steps: 1277475, episodes: 51100, mean episode reward: -653.712, reward: -9.344, time: 5.29, TD error: 7.869, c_model: 4.073, actorQ: 29.627, a_model: 0.07
steps: 1279975, episodes: 51200, mean episode reward: -651.839, reward: -9.637, time: 5.327, TD error: 7.861, c_model: 4.068, actorQ: 29.649, a_model: 0.07
steps: 1282475, episodes: 51300, mean episode reward: -679.089, reward: -10.325, time: 5.282, TD error: 7.853, c_model: 4.064, actorQ: 29.676, a_model: 0.07
steps: 1284975, episodes: 51400, mean episode reward: -666.92, reward: -9.988, time: 5.21, TD error: 7.847, c_model: 4.061, actorQ: 29.7, a_model: 0.07
steps: 1287475, episodes: 51500, mean episode reward: -658.787, reward: -9.582, time: 5.202, TD error: 7.847, c_model: 4.06, actorQ: 29.725, a_model: 0.07
steps: 1289975, episodes: 51600, mean episode reward: -671.103, reward: -10.163, time: 5.278, TD error: 7.842, c_model: 4.057, actorQ: 29.753, a_model: 0.069
steps: 1292475, episodes: 51700, mean episode reward: -666.127, reward: -9.832, time: 5.317, TD error: 7.835, c_model: 4.053, actorQ: 29.791, a_model: 0.069
steps: 1294975, episodes: 51800, mean episode reward: -662.151, reward: -9.786, time: 5.239, TD error: 7.829, c_model: 4.049, actorQ: 29.817, a_model: 0.069
steps: 1297475, episodes: 51900, mean episode reward: -668.751, reward: -10.214, time: 5.226, TD error: 7.822, c_model: 4.046, actorQ: 29.845, a_model: 0.069
steps: 1299975, episodes: 52000, mean episode reward: -696.935, reward: -10.488, time: 5.244, TD error: 7.815, c_model: 4.042, actorQ: 29.865, a_model: 0.069
steps: 1302475, episodes: 52100, mean episode reward: -686.094, reward: -10.563, time: 5.222, TD error: 7.81, c_model: 4.039, actorQ: 29.885, a_model: 0.069
steps: 1304975, episodes: 52200, mean episode reward: -658.649, reward: -10.112, time: 5.231, TD error: 7.803, c_model: 4.034, actorQ: 29.908, a_model: 0.069
steps: 1307475, episodes: 52300, mean episode reward: -654.826, reward: -9.56, time: 5.287, TD error: 7.802, c_model: 4.034, actorQ: 29.945, a_model: 0.069
steps: 1309975, episodes: 52400, mean episode reward: -658.841, reward: -9.881, time: 5.399, TD error: 7.795, c_model: 4.03, actorQ: 29.969, a_model: 0.069
steps: 1312475, episodes: 52500, mean episode reward: -665.633, reward: -9.891, time: 5.232, TD error: 7.785, c_model: 4.025, actorQ: 29.991, a_model: 0.069
steps: 1314975, episodes: 52600, mean episode reward: -682.408, reward: -10.288, time: 5.23, TD error: 7.784, c_model: 4.024, actorQ: 30.018, a_model: 0.069
steps: 1317475, episodes: 52700, mean episode reward: -659.452, reward: -10.404, time: 5.211, TD error: 7.775, c_model: 4.019, actorQ: 30.038, a_model: 0.069
steps: 1319975, episodes: 52800, mean episode reward: -701.859, reward: -10.903, time: 5.272, TD error: 7.767, c_model: 4.014, actorQ: 30.057, a_model: 0.069
steps: 1322475, episodes: 52900, mean episode reward: -666.092, reward: -9.984, time: 5.291, TD error: 7.76, c_model: 4.01, actorQ: 30.09, a_model: 0.069
steps: 1324975, episodes: 53000, mean episode reward: -670.526, reward: -9.872, time: 5.253, TD error: 7.757, c_model: 4.009, actorQ: 30.118, a_model: 0.069
steps: 1327475, episodes: 53100, mean episode reward: -661.914, reward: -9.729, time: 5.26, TD error: 7.75, c_model: 4.005, actorQ: 30.138, a_model: 0.069
steps: 1329975, episodes: 53200, mean episode reward: -657.21, reward: -9.902, time: 5.239, TD error: 7.744, c_model: 4.001, actorQ: 30.159, a_model: 0.069
steps: 1332475, episodes: 53300, mean episode reward: -657.554, reward: -9.516, time: 5.219, TD error: 7.741, c_model: 3.999, actorQ: 30.187, a_model: 0.069
steps: 1334975, episodes: 53400, mean episode reward: -675.701, reward: -9.796, time: 5.229, TD error: 7.737, c_model: 3.997, actorQ: 30.216, a_model: 0.068
steps: 1337475, episodes: 53500, mean episode reward: -662.539, reward: -9.857, time: 5.292, TD error: 7.73, c_model: 3.992, actorQ: 30.245, a_model: 0.068
steps: 1339975, episodes: 53600, mean episode reward: -672.178, reward: -10.259, time: 5.233, TD error: 7.723, c_model: 3.989, actorQ: 30.266, a_model: 0.068
steps: 1342475, episodes: 53700, mean episode reward: -671.841, reward: -10.198, time: 5.222, TD error: 7.716, c_model: 3.984, actorQ: 30.288, a_model: 0.068
steps: 1344975, episodes: 53800, mean episode reward: -662.911, reward: -9.912, time: 5.234, TD error: 7.71, c_model: 3.981, actorQ: 30.306, a_model: 0.068
steps: 1347475, episodes: 53900, mean episode reward: -682.351, reward: -10.333, time: 5.257, TD error: 7.703, c_model: 3.977, actorQ: 30.329, a_model: 0.068
steps: 1349975, episodes: 54000, mean episode reward: -667.864, reward: -10.202, time: 5.234, TD error: 7.697, c_model: 3.974, actorQ: 30.353, a_model: 0.068
steps: 1352475, episodes: 54100, mean episode reward: -647.305, reward: -9.563, time: 5.283, TD error: 7.697, c_model: 3.974, actorQ: 30.39, a_model: 0.068
steps: 1354975, episodes: 54200, mean episode reward: -682.202, reward: -10.34, time: 5.484, TD error: 7.691, c_model: 3.971, actorQ: 30.412, a_model: 0.068
steps: 1357475, episodes: 54300, mean episode reward: -673.843, reward: -10.378, time: 5.277, TD error: 7.683, c_model: 3.966, actorQ: 30.43, a_model: 0.068
steps: 1359975, episodes: 54400, mean episode reward: -653.597, reward: -9.614, time: 5.227, TD error: 7.675, c_model: 3.961, actorQ: 30.449, a_model: 0.068
steps: 1362475, episodes: 54500, mean episode reward: -663.285, reward: -9.99, time: 5.262, TD error: 7.67, c_model: 3.959, actorQ: 30.474, a_model: 0.068
steps: 1364975, episodes: 54600, mean episode reward: -657.61, reward: -9.961, time: 5.268, TD error: 7.664, c_model: 3.956, actorQ: 30.499, a_model: 0.068
steps: 1367475, episodes: 54700, mean episode reward: -681.425, reward: -10.129, time: 5.33, TD error: 7.661, c_model: 3.954, actorQ: 30.531, a_model: 0.068
steps: 1369975, episodes: 54800, mean episode reward: -664.76, reward: -9.924, time: 5.271, TD error: 7.658, c_model: 3.952, actorQ: 30.556, a_model: 0.068
steps: 1372475, episodes: 54900, mean episode reward: -658.402, reward: -9.743, time: 5.269, TD error: 7.651, c_model: 3.949, actorQ: 30.581, a_model: 0.068
steps: 1374975, episodes: 55000, mean episode reward: -643.248, reward: -9.485, time: 5.549, TD error: 7.644, c_model: 3.945, actorQ: 30.604, a_model: 0.068
steps: 1377475, episodes: 55100, mean episode reward: -649.689, reward: -9.977, time: 6.036, TD error: 7.641, c_model: 3.942, actorQ: 30.628, a_model: 0.068
steps: 1379975, episodes: 55200, mean episode reward: -666.09, reward: -10.293, time: 5.629, TD error: 7.636, c_model: 3.939, actorQ: 30.652, a_model: 0.068
steps: 1382475, episodes: 55300, mean episode reward: -667.295, reward: -10.201, time: 5.346, TD error: 7.629, c_model: 3.935, actorQ: 30.682, a_model: 0.068
steps: 1384975, episodes: 55400, mean episode reward: -665.523, reward: -9.903, time: 5.296, TD error: 7.626, c_model: 3.933, actorQ: 30.702, a_model: 0.068
steps: 1387475, episodes: 55500, mean episode reward: -657.535, reward: -9.812, time: 5.311, TD error: 7.62, c_model: 3.929, actorQ: 30.727, a_model: 0.068
steps: 1389975, episodes: 55600, mean episode reward: -642.536, reward: -9.515, time: 5.228, TD error: 7.615, c_model: 3.927, actorQ: 30.752, a_model: 0.068
steps: 1392475, episodes: 55700, mean episode reward: -671.734, reward: -10.103, time: 5.232, TD error: 7.608, c_model: 3.923, actorQ: 30.776, a_model: 0.067
steps: 1394975, episodes: 55800, mean episode reward: -658.775, reward: -9.701, time: 5.222, TD error: 7.607, c_model: 3.921, actorQ: 30.806, a_model: 0.067
steps: 1397475, episodes: 55900, mean episode reward: -667.387, reward: -9.946, time: 5.306, TD error: 7.601, c_model: 3.917, actorQ: 30.838, a_model: 0.067
steps: 1399975, episodes: 56000, mean episode reward: -644.972, reward: -9.512, time: 5.259, TD error: 7.596, c_model: 3.915, actorQ: 30.86, a_model: 0.067
steps: 1402475, episodes: 56100, mean episode reward: -671.305, reward: -9.751, time: 5.27, TD error: 7.59, c_model: 3.91, actorQ: 30.884, a_model: 0.067
steps: 1404975, episodes: 56200, mean episode reward: -635.125, reward: -9.546, time: 5.23, TD error: 7.588, c_model: 3.909, actorQ: 30.908, a_model: 0.067
steps: 1407475, episodes: 56300, mean episode reward: -652.281, reward: -9.527, time: 5.209, TD error: 7.581, c_model: 3.905, actorQ: 30.934, a_model: 0.067
steps: 1409975, episodes: 56400, mean episode reward: -655.786, reward: -9.64, time: 5.244, TD error: 7.575, c_model: 3.902, actorQ: 30.96, a_model: 0.067
steps: 1412475, episodes: 56500, mean episode reward: -655.969, reward: -9.521, time: 5.285, TD error: 7.564, c_model: 3.896, actorQ: 30.989, a_model: 0.067
steps: 1414975, episodes: 56600, mean episode reward: -663.01, reward: -9.733, time: 5.254, TD error: 7.563, c_model: 3.896, actorQ: 31.012, a_model: 0.067
steps: 1417475, episodes: 56700, mean episode reward: -668.77, reward: -9.96, time: 5.253, TD error: 7.56, c_model: 3.894, actorQ: 31.035, a_model: 0.067
steps: 1419975, episodes: 56800, mean episode reward: -648.24, reward: -9.723, time: 5.256, TD error: 7.553, c_model: 3.89, actorQ: 31.057, a_model: 0.067
steps: 1422475, episodes: 56900, mean episode reward: -685.288, reward: -10.413, time: 5.242, TD error: 7.553, c_model: 3.889, actorQ: 31.084, a_model: 0.067
steps: 1424975, episodes: 57000, mean episode reward: -674.906, reward: -10.277, time: 5.243, TD error: 7.549, c_model: 3.887, actorQ: 31.107, a_model: 0.067
steps: 1427475, episodes: 57100, mean episode reward: -668.721, reward: -9.922, time: 5.259, TD error: 7.543, c_model: 3.883, actorQ: 31.135, a_model: 0.067
steps: 1429975, episodes: 57200, mean episode reward: -660.943, reward: -9.784, time: 5.25, TD error: 7.535, c_model: 3.879, actorQ: 31.156, a_model: 0.067
steps: 1432475, episodes: 57300, mean episode reward: -673.243, reward: -9.905, time: 5.232, TD error: 7.528, c_model: 3.875, actorQ: 31.176, a_model: 0.067
steps: 1434975, episodes: 57400, mean episode reward: -663.614, reward: -9.575, time: 5.255, TD error: 7.524, c_model: 3.873, actorQ: 31.199, a_model: 0.067
steps: 1437475, episodes: 57500, mean episode reward: -645.461, reward: -9.442, time: 5.212, TD error: 7.52, c_model: 3.87, actorQ: 31.229, a_model: 0.067
steps: 1439975, episodes: 57600, mean episode reward: -666.298, reward: -9.981, time: 5.293, TD error: 7.512, c_model: 3.866, actorQ: 31.25, a_model: 0.067
steps: 1442475, episodes: 57700, mean episode reward: -662.783, reward: -9.8, time: 5.314, TD error: 7.509, c_model: 3.863, actorQ: 31.281, a_model: 0.067
steps: 1444975, episodes: 57800, mean episode reward: -675.193, reward: -10.248, time: 5.251, TD error: 7.503, c_model: 3.86, actorQ: 31.305, a_model: 0.067
steps: 1447475, episodes: 57900, mean episode reward: -672.628, reward: -10.244, time: 5.196, TD error: 7.495, c_model: 3.856, actorQ: 31.326, a_model: 0.067
steps: 1449975, episodes: 58000, mean episode reward: -664.559, reward: -9.862, time: 5.342, TD error: 7.487, c_model: 3.851, actorQ: 31.353, a_model: 0.067
steps: 1452475, episodes: 58100, mean episode reward: -660.177, reward: -9.756, time: 5.251, TD error: 7.482, c_model: 3.849, actorQ: 31.381, a_model: 0.067
steps: 1454975, episodes: 58200, mean episode reward: -692.704, reward: -10.618, time: 5.285, TD error: 7.475, c_model: 3.845, actorQ: 31.401, a_model: 0.067
steps: 1457475, episodes: 58300, mean episode reward: -647.154, reward: -9.694, time: 5.461, TD error: 7.468, c_model: 3.84, actorQ: 31.439, a_model: 0.067
steps: 1459975, episodes: 58400, mean episode reward: -656.167, reward: -9.822, time: 5.256, TD error: 7.463, c_model: 3.837, actorQ: 31.466, a_model: 0.067
steps: 1462475, episodes: 58500, mean episode reward: -661.432, reward: -10.186, time: 5.272, TD error: 7.455, c_model: 3.833, actorQ: 31.491, a_model: 0.067
steps: 1464975, episodes: 58600, mean episode reward: -680.364, reward: -9.947, time: 5.22, TD error: 7.446, c_model: 3.828, actorQ: 31.514, a_model: 0.067
steps: 1467475, episodes: 58700, mean episode reward: -643.743, reward: -9.088, time: 5.253, TD error: 7.44, c_model: 3.824, actorQ: 31.54, a_model: 0.067
steps: 1469975, episodes: 58800, mean episode reward: -646.004, reward: -9.277, time: 5.256, TD error: 7.435, c_model: 3.821, actorQ: 31.561, a_model: 0.067
steps: 1472475, episodes: 58900, mean episode reward: -625.011, reward: -9.225, time: 5.343, TD error: 7.426, c_model: 3.816, actorQ: 31.591, a_model: 0.067
steps: 1474975, episodes: 59000, mean episode reward: -633.302, reward: -9.008, time: 5.262, TD error: 7.422, c_model: 3.814, actorQ: 31.619, a_model: 0.067
steps: 1477475, episodes: 59100, mean episode reward: -651.213, reward: -9.446, time: 5.244, TD error: 7.414, c_model: 3.81, actorQ: 31.64, a_model: 0.067
steps: 1479975, episodes: 59200, mean episode reward: -636.176, reward: -8.916, time: 5.238, TD error: 7.409, c_model: 3.807, actorQ: 31.663, a_model: 0.067
steps: 1482475, episodes: 59300, mean episode reward: -673.602, reward: -10.142, time: 5.233, TD error: 7.401, c_model: 3.802, actorQ: 31.679, a_model: 0.066
steps: 1484975, episodes: 59400, mean episode reward: -633.802, reward: -9.239, time: 5.295, TD error: 7.394, c_model: 3.797, actorQ: 31.701, a_model: 0.066
steps: 1487475, episodes: 59500, mean episode reward: -654.06, reward: -9.625, time: 5.335, TD error: 7.389, c_model: 3.795, actorQ: 31.733, a_model: 0.066
steps: 1489975, episodes: 59600, mean episode reward: -636.121, reward: -9.111, time: 5.285, TD error: 7.384, c_model: 3.791, actorQ: 31.753, a_model: 0.066
steps: 1492475, episodes: 59700, mean episode reward: -657.993, reward: -9.379, time: 5.225, TD error: 7.38, c_model: 3.789, actorQ: 31.781, a_model: 0.066
steps: 1494975, episodes: 59800, mean episode reward: -651.903, reward: -9.407, time: 5.25, TD error: 7.373, c_model: 3.785, actorQ: 31.806, a_model: 0.066
steps: 1497475, episodes: 59900, mean episode reward: -642.121, reward: -9.306, time: 5.263, TD error: 7.364, c_model: 3.78, actorQ: 31.83, a_model: 0.066
steps: 1499975, episodes: 60000, mean episode reward: -629.875, reward: -9.097, time: 5.299, TD error: 7.353, c_model: 3.774, actorQ: 31.851, a_model: 0.066
